import unicodedata
import logging
import re
from sentence_transformers import CrossEncoder
from typing import List, Optional, Tuple
from sentence_transformers import SentenceTransformer
from app.core.config_loader import get_heritage_config

logger = logging.getLogger("uvicorn")

class KnowledgeBase:
    def __init__(self, v_db, embedder: SentenceTransformer):
        """
        Kh·ªüi t·∫°o KnowledgeBase v·ªõi vector database v√† sentence transformer (embedder).
        """
        self.v_db = v_db
        self.embedder = embedder
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.history = []

        # ‚≠ê Graph Store (Hybrid RAG)
        from app.core.graph_store import GraphStore
        self.graph = GraphStore(v_db.db) if v_db.db is not None else None
        logger.info("‚úÖ [KnowledgeBase] GraphStore initialized.")

        # [NEW] Semantic Routing Index (In-Memory)
        self.route_data = []
        self.route_embeddings = None
        self._build_routing_index()

        self.reload_config()
        
    def reload_config(self):
        """
        Reload l·∫°i c·∫•u h√¨nh t·ª´ kh√≥a t·ª´ data_manager.
        """
        from app.core.config_loader import get_heritage_config
        config = get_heritage_config()
        self.site_keywords = {}
        for key, data in config.items():
            # Gom keyword: name + context keys (n·∫øu c√≥)
            # T·∫°m th·ªùi ch·ªâ l·∫•y name l√†m keyword ch√≠nh
            keywords = [data["name"].lower()]
            # N·∫øu mu·ªën t√°ch name th√†nh c√°c t·ª´ kh√≥a ph·ª•, x·ª≠ l√Ω th√™m ·ªü ƒë√¢y
            # V√≠ d·ª•: "VƒÉn Mi·∫øu Qu·ªëc T·ª≠ Gi√°m" -> ["vƒÉn mi·∫øu", "qu·ªëc t·ª≠ gi√°m"]
            # Logic ƒë∆°n gi·∫£n: th√™m key v√†o
            keywords.append(key.replace("_", " "))
            
            self.site_keywords[key] = keywords
            
        logger.info(f"üîÑ KnowledgeBase reloaded. Sites: {list(self.site_keywords.keys())}")

    def _build_routing_index(self):
        """
        Kh·ªüi t·∫°o index cho vi·ªác ƒë·ªãnh tuy·∫øn semantic.
        ƒê·ªçc config, embed descriptions v√† l∆∞u v√†o RAM.
        """
        try:
            from app.core.config_loader import get_heritage_config
            config = get_heritage_config()
            
            texts = []
            self.route_data = []
            
            for key, data in config.items():
                # K·∫øt h·ª£p T√™n + M√¥ t·∫£ ƒë·ªÉ embed
                desc = f"{data['name']} {data.get('context_description', '')}"
                texts.append(desc)
                self.route_data.append(data)
            
            if texts:
                # Embed batch
                logger.info(f"üîÑ Building Routing Index for {len(texts)} sites...")
                self.route_embeddings = self.embedder.encode(texts, convert_to_tensor=True)
                logger.info("‚úÖ Routing Index Ready!")
            else:
                self.route_embeddings = None
                
        except Exception as e:
            logger.error(f"‚ùå Failed to build routing index: {e}")

    def find_potential_sites(self, query: str, top_k: int = 3) -> List[dict]:
        """
        T√¨m top_k di t√≠ch kh·ªõp ng·ªØ nghƒ©a nh·∫•t v·ªõi c√¢u h·ªèi.
        D√πng cosine similarity.
        """
        if self.route_embeddings is None or not self.route_data:
            return []
            
        from sentence_transformers import util
        t0 = logging.time.time()
        
        # Embed query
        q_vec = self.embedder.encode(query, convert_to_tensor=True)
        
        # Cosine Similarity
        # (1, D) x (N, D).T -> (1, N)
        scores = util.cos_sim(q_vec, self.route_embeddings)[0]
        
        # Get Top K
        # torch.topk tr·∫£ v·ªÅ (values, indices)
        top_results = scores.topk(k=min(top_k, len(self.route_data)))
        
        results = []
        for score, idx in zip(top_results.values, top_results.indices):
            if score.item() > 0.3: # Threshold
                site_info = self.route_data[idx.item()]
                results.append(site_info)
                logger.info(f"   üîç Route Match: {site_info['name']} (Score: {score.item():.2f})")
                
        # logger.info(f"‚è±Ô∏è Routing took: {logging.time.time() - t0:.3f}s")
        return results
    def normalize_query(self, query: str) -> str:
        """
        [STEP 1] Normalize Input: Chu·∫©n h√≥a text ƒë·∫ßu v√†o.
        """
        import re
        if not query: return ""
        # Lowercase, b·ªè kho·∫£ng tr·∫Øng th·ª´a
        q = query.lower().strip()
        q = re.sub(r'\s+', ' ', q)
        return q

    def _keyword_boost(self, text, query):
        """
        Helper for Hybrid Keyword Scoring.
        """
        def norm(t): 
            return "".join(c for c in unicodedata.normalize('NFD', t.lower()) 
                          if unicodedata.category(c) != 'Mn')
        t_c, q_c = norm(text), norm(query)
        q_w = [w for w in q_c.split() if len(w) > 2]
        if not q_w: return 0
        return sum(1 for w in q_w if w in t_c) / len(q_w)

    # ... (Gi·ªØ nguy√™n c√°c h√†m detect_gibberish, resolve_pronoun) ...

    async def fetch_and_rerank(self, query: str, site_key: str, history: List[dict] = None):
        """
        [STEP 5] Hybrid Retrieve + Rerank
        - Retrieve: Vector Search (Semantic) from specified collection in config
        - Rerank: Cross-Encoder + Keyword Boost
        """
        # Load config for site to get collection and filter
        from app.core.config_loader import get_site_config
        site_config = get_site_config(site_key)
        
        if not site_config:
            logger.warning(f"‚ö†Ô∏è Site key '{site_key}' not found in config. Defaulting to 'culture'.")
            collection_name = "culture"
            filter_dict = {} # Global search fallback
        else:
            collection_name = site_config.get("collection", "culture") # Default to culture if missing
            filter_dict = site_config.get("filter", {}) # Default to empty filter if missing
            
        # [STATELESS] Kh√¥ng l∆∞u history v√†o self.history ƒë·ªÉ h·ªó tr·ª£ multi-user
        # History ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ request context n·∫øu c·∫ßn d√πng cho retrieval metadata
        if history:
             pass # Logic x·ª≠ l√Ω history n·∫øu c·∫ßn (vd: preference profile)

        # T·∫°o vector cho c√¢u h·ªèi
        # üîß FIX CASE SENSITIVITY: 
        # N·∫øu query to√†n ch·ªØ th∆∞·ªùng (vd: 'ng√¥ th√¨ nh·∫≠m'), vector c√≥ th·ªÉ l·ªách so v·ªõi 'Ng√¥ Th√¨ Nh·∫≠m' trong DB.
        # Ta th·ª≠ Auto-Capitalize (Title Case) ƒë·ªÉ b·∫Øt t√™n ri√™ng t·ªët h∆°n.
        queries_to_embed = [query]
        if query.islower():
            queries_to_embed.append(query.title()) # Th√™m phi√™n b·∫£n vi·∫øt hoa: "Ng√¥ Th√¨ Nh·∫≠m"
        
        # L·∫•y vector trung b√¨nh ho·∫∑c d√πng vector t·ªët nh·∫•t? 
        # ƒê∆°n gi·∫£n: ∆Øu ti√™n d√πng phi√™n b·∫£n Title Case n·∫øu c√≥ v·∫ª l√† t√™n ri√™ng.
        # Ho·∫∑c search 2 l·∫ßn merge k·∫øt qu·∫£.
        
        # C√ÅCH 2: Search b·∫±ng b·∫£n Title Case lu√¥n n·∫øu query ng·∫Øn (<10 t·ª´) v√† lowercase
        target_query = query
        if query.islower() and len(query.split()) < 10:
             target_query = query.title()

        q_vec = self.embedder.encode([target_query])[0].tolist()
        
        # [FIX] Filter is already loaded from config above
        # filter_dict = {"metadata.site_key": site_key} if site_key else {}
        
        # === LOGGING CHI TI·∫æT ===
        logger.info(f"üìö [Knowledge Base] Retrieval Details:")
        logger.info(f"   ‚îú‚îÄ Collection: '{collection_name}'")
        logger.info(f"   ‚îú‚îÄ Filter: {filter_dict if filter_dict else 'None (Global Search)'}")
        logger.info(f"   ‚îú‚îÄ Query (processed): '{target_query}'")
        logger.info(f"   ‚îî‚îÄ Limit: 15 candidates")

        # 1. Retrieve Candidates (Vector Search) - PHASE 1: Strict Site Filter
        candidates = await self.query(
            collection_name=collection_name,  # Dynamic collection
            query_vector=q_vec, 
            limit=15, 
            filter_dict=filter_dict
        )
        
        logger.info(f"   ‚úÖ Retrieved {len(candidates)} candidates t·ª´ '{collection_name}' (Filter Strict)")

        # [NEW] PHASE 1.5: Fallback Unfiltered Search on SAME Collection
        # N·∫øu filter tr·∫£ v·ªÅ 0 (c√≥ th·ªÉ do l·ªói Index ch∆∞a config filter field), th·ª≠ t√¨m kh√¥ng filter
        if len(candidates) == 0 and site_key:
             logger.warning(f"‚ö†Ô∏è Filter Search tr·∫£ v·ªÅ 0. Th·ª≠ t√¨m KH√îNG filter tr√™n '{collection_name}' (Check l·ªói Index Atlas)...")
             unfiltered_candidates = await self.query(
                collection_name=collection_name,
                query_vector=q_vec,
                limit=10,
                filter_dict={} # Remove filter
             )
             # Post-filter b·∫±ng Python (n·∫øu collection h·ªón t·∫°p)
             # Tuy nhi√™n n·∫øu collection chuy√™n bi·ªát (heritage ch·ªâ c√≥ heritage) th√¨ oke.
             # N·∫øu collection chung chung, ta c·∫ßn check metadata.
             filtered_in_memory = []
             for c in unfiltered_candidates:
                 # Check 'heritage_type' or 'culture_type' field matches site_key
                 # Ho·∫∑c check metadata.site_key
                 c_site = c.get('metadata', {}).get('site_key')
                 # Dynamic field check
                 dyna_key = None
                 if 'heritage_type' in c: dyna_key = c['heritage_type']
                 elif 'culture_type' in c: dyna_key = c['culture_type']
                 
                 if c_site == site_key or dyna_key == site_key:
                     filtered_in_memory.append(c)
             
             if filtered_in_memory:
                 logger.info(f"   ‚úÖ T√¨m th·∫•y {len(filtered_in_memory)} chunks khi b·ªè Index Filter (L·ªói c·∫•u h√¨nh Atlas!)")
                 candidates.extend(filtered_in_memory)
             else:
                 logger.info(f"   ‚ùå V·∫´n kh√¥ng t√¨m th·∫•y g√¨ tr√™n '{collection_name}' k·ªÉ c·∫£ khi b·ªè filter.")

        # PHASE 2: Global Fallback (N·∫øu t√¨m trong site kh√¥ng th·∫•y, t√¨m to√†n b·ªô kho)
        # N·∫øu filter filters qu√° ch·∫∑t l√†m m·∫•t data (v√≠ d·ª• C·ªôt C·ªù n·∫±m file ri√™ng nh∆∞ng user h·ªèi Ho√†ng Th√†nh)
        if len(candidates) < 3 and site_key: 
            logger.info(f"‚ö†Ô∏è [FALLBACK PHASE 2] √çt k·∫øt qu·∫£ ({len(candidates)} chunks). M·ªü r·ªông ‚Üí Global Search (Multi-Collection)...")
            
            # List of collections to search
            fallback_cols = ["heritage", "culture", "history", "sites"]
            # Exclude current primary collection to avoid redundant search
            fallback_cols = [c for c in fallback_cols if c != collection_name]
            
            import asyncio
            # Run queries concurrently using self.query (async wrapper)
            tasks = [
                self.query(col, q_vec, limit=5, filter_dict={}) 
                for col in fallback_cols
            ]
            results_list = await asyncio.gather(*tasks)
            
            added_count = 0
            existing_ids = {c.get('id', str(c.get('content'))) for c in candidates} # Use content as fallback ID
            
            for res_batch, col_name in zip(results_list, fallback_cols):
                 for gc in res_batch:
                    # Simple dedupe
                    chk = gc.get('id', gc.get('content'))
                    if chk not in existing_ids:
                        # Mark source collection for debugging
                        if 'metadata' not in gc: gc['metadata'] = {}
                        gc['metadata']['fallback_source'] = col_name
                        candidates.append(gc)
                        existing_ids.add(chk)
                        added_count += 1
            
            logger.info(f"   ‚úÖ Th√™m {added_count} chunks t·ª´ Global Search. T·ªïng: {len(candidates)}")
        if len(candidates) < 1:
            logger.info(f"‚ö†Ô∏è [FALLBACK PHASE 3] Kh√¥ng c√≥ k·∫øt qu·∫£ Vector Search. Th·ª≠ Regex...")
            try:
                # T·∫°o regex query: "ngo thi nham" -> "ngo.*thi.*nham"
                # Ch·ªâ √°p d·ª•ng n·∫øu query ng·∫Øn (< 5 t·ª´) tr√°nh regex qu√° d√†i ch·∫≠m DB
                simple_q = query.lower()
                clean_q = re.sub(r'[^\w\s]', '', simple_q).strip() 
                if len(clean_q.split()) < 6:
                    regex_pat = ".*".join(clean_q.split())
                    
                    # Regex across collections? No, just primary + heritage/culture as backup.
                    regex_cols = [collection_name]
                    if collection_name != "culture": regex_cols.append("culture")
                    if collection_name != "heritage": regex_cols.append("heritage")
                    
                    # Deduplicate collections
                    regex_cols = list(set(regex_cols))
                    logger.info(f"   ‚Üí Regex Pattern: '{regex_pat}' on {regex_cols}")
                    
                    for col in regex_cols:
                        regex_candidates = self.v_db.find_regex(
                            collection_name=col,
                            regex_pattern=regex_pat,
                            limit=2
                        )
                        for rc in regex_candidates:
                           rc['score'] = 0.4 # Default score for regex match
                           if not any(c.get('content') == rc['content'] for c in candidates):
                                 candidates.append(rc)
            except Exception as e:
                logger.error(f"Regex Search Error: {e}")

        if not candidates:
            logger.warning(f"‚ùå Kh√¥ng t√¨m th·∫•y chunks n√†o (t·∫•t c·∫£ fallback ƒë·ªÅu fail)")
            return None # Return None ƒë·ªÉ agentic workflow k√≠ch ho·∫°t LLM Fallback

        # 2. Rerank (Hybrid: Semantic Score + Keyword Match)
        # Fix: V·ªõi Regex candidate (kh√¥ng c√≥ 'id'), c√≥ th·ªÉ g√¢y l·ªói reranker predict n·∫øu data l·∫°.
        # Clean candidates list
        valid_candidates = [c for c in candidates if 'content' in c]
        
        if not valid_candidates:
            return None
        
        logger.info(f"üîÑ [RERANK] Processing {len(valid_candidates)} candidates...")
        pairs = [[query, c['content']] for c in valid_candidates]
        scores = self.reranker.predict(pairs)

        for i, res in enumerate(valid_candidates):
            k_boost = self._keyword_boost(res['content'], query)
            # Adjust score calculation
            base_score = scores[i]
            res['final_score'] = (base_score * 0.7) + (k_boost * 0.3)

        valid_candidates.sort(key=lambda x: x['final_score'], reverse=True)
        
        # Log top 3
        logger.info(f"   üìä Top 3 Results:")
        for i, c in enumerate(valid_candidates[:3]):
            source = c.get('metadata', {}).get('source', 'unknown')
            score = c.get('final_score', 0)
            preview = c['content'][:80].replace('\n', ' ')
            logger.info(f"      {i+1}. Score={score:.3f} | Source={source} | '{preview}...'")

        # [STRICT FILTER] Ki·ªÉm tra ƒëi·ªÉm s·ªë c·ªßa Top 1
        if not valid_candidates:
            return None
            
        top_score = valid_candidates[0]['final_score']
        # Ng∆∞·ª°ng ch·∫∑n (Threshold):
        # CrossEncoder score th∆∞·ªùng l√† logit.
        # Match t·ªët: > 0.Match kh√°: > -2. Match t·ªá: < -4.
        # Set ng∆∞·ª°ng -2.0 ƒë·ªÉ an to√†n (M√∫a r·ªëi vs Lam S∆°n ra -6.x -> S·∫Ω b·ªã ch·∫∑n)
        MIN_SCORE_THRESHOLD = -2.0
        
        if top_score < MIN_SCORE_THRESHOLD:
            logger.warning(f"‚ö†Ô∏è [STRICT MODE] Top 1 score ({top_score:.3f}) qu√° th·∫•p (< {MIN_SCORE_THRESHOLD}). Coi nh∆∞ kh√¥ng t√¨m th·∫•y.")
            return None

        answer = "\n\n".join([c['content'] for c in valid_candidates[:3]])
        
        logger.info(f"‚úÖ [HERITAGE RAG] Tr·∫£ v·ªÅ {len(answer)} k√Ω t·ª± context t·ª´ top-3 chunks")

        # ‚≠ê GRAPH EXPANSION: Expand context b·∫±ng Knowledge Graph
        graph_context = self._graph_expand(query, site_key)
        if graph_context:
            answer = answer + "\n\n" + graph_context
            logger.info(f"üï∏Ô∏è  [Graph] Appended {len(graph_context)} chars graph context")

        return answer

    def _graph_expand(self, query: str, site_key: Optional[str] = None) -> str:
        """
        ‚≠ê Hybrid RAG Graph Layer:
        1. T√¨m entities trong query
        2. Query knowledge_graph collection
        3. Format th√†nh text ƒë∆∞a v√†o LLM context
        """
        if self.graph is None:
            return ""

        try:
            site_triples = []
            entity_triples = []

            # B1: L·∫•y triples c·ªßa site hi·ªán t·∫°i (breadth)
            if site_key:
                site_triples = self.graph.get_by_site(site_key, limit=20)
                logger.info(f"   üï∏Ô∏è  [Graph B1] Site '{site_key}': {len(site_triples)} triples")

            # B2: T√¨m entities trong query ‚Äî d√πng c√°ch t√°ch t·ª´ ƒë∆°n gi·∫£n h∆°n regex
            # T√°ch query th√†nh c√°c c·ª•m t·ª´ 2-4 t·ª´ li√™n ti·∫øp
            words = query.split()
            candidates = []
            for n in [3, 2, 4]:  # ∆Øu ti√™n c·ª•m 3 t·ª´, r·ªìi 2, r·ªìi 4
                for i in range(len(words) - n + 1):
                    phrase = " ".join(words[i:i+n])
                    # Lo·∫°i b·ªè c·ª•m c√≥ t·ª´ n·ªëi ph·ªï bi·∫øn
                    skip_words = {"l√†", "v√†", "c√≥", "g√¨", "c·ªßa", "ƒë·∫øn", "t·ª´", "trong", "v·ªõi", "kh√¥ng", "n√†o", "ƒë∆∞·ª£c"}
                    phrase_words = set(phrase.lower().split())
                    if len(phrase) >= 4 and not phrase_words.issubset(skip_words):
                        candidates.append(phrase)

            # Deduplicate v√† l·∫•y max 5
            seen = set()
            unique_candidates = []
            for c in candidates:
                if c not in seen:
                    seen.add(c)
                    unique_candidates.append(c)

            logger.info(f"   üï∏Ô∏è  [Graph B2] Entity candidates: {unique_candidates[:5]}")
            for entity in unique_candidates[:5]:
                et = self.graph.get_neighbors(entity, depth=2, max_nodes=8)
                if et:
                    logger.info(f"      ‚Üí '{entity}': +{len(et)} triples")
                entity_triples.extend(et)

            # Merge: Entity-specific triples TR∆Ø·ªöC (li√™n quan h∆°n), site triples sau
            triples = entity_triples + site_triples

            if not triples:
                logger.info(f"   üï∏Ô∏è  [Graph] No triples ‚Üí skip")
                return ""

            formatted = self.graph.format_triples_as_context(triples)
            if not formatted:
                return ""

            # Log 3 sample triples ƒë·∫ßu (b√¢y gi·ªù l√† entity triples)
            sample_lines = formatted.strip().split("\n")[:3]
            logger.info(f"   üï∏Ô∏è  [Graph] {len(triples)} triples | Top sample:")
            for line in sample_lines:
                logger.info(f"      {line.strip()}")

            return f"\nüîó M·ªêI QUAN H·ªÜ (Knowledge Graph):\n{formatted}"

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [Graph] Expand error (non-critical): {e}")
            return ""

    def detect_gibberish_query(self, query: str, history: List[dict] = None) -> Tuple[bool, Optional[str]]:
        """
        üîß NEW: Detect c√¢u gibberish (ng·∫Øn, v√¥ nghƒ©a, kh√¥ng r√µ √Ω)
        """
        query_lower = query.lower().strip()
        history = history or [] # Safe fallback
        
        # N·∫øu c√¢u qu√° ng·∫Øn (e.g., "hdpp", "v.v", "chi chi", "g√¨ g√¨")
        if len(query_lower) < 10:
            # Ki·ªÉm tra n·∫øu l√† nh·ªØng t·ª´ th∆∞·ªùng d√πng trong gibberish
            gibberish_patterns = [
                "hdpp", "v.v", "vv", "v√¢n v√¢n", "v", "chi", "g√¨ g√¨", "kh√°yy", "chi chi",
                "th·∫ø n√†o", "t·∫°i sao", "nh∆∞ n√†o", "c√°i g√¨", "c√°i chi", "ai", "c√¥", "c√°i",
                "√†", "∆°i", "∆°i", "n√†y", "kia", "k√¨a"
            ]
            
            is_gibberish = any(pattern in query_lower for pattern in gibberish_patterns)
            
            if is_gibberish and history:
                # L·∫•y topic t·ª´ c√¢u h·ªèi tr∆∞·ªõc
                prev_entry = history[-1] if history else None
                if prev_entry:
                    prev_question = prev_entry.get('user_input', '')
                    prev_site = prev_entry.get('site')
                    logger.info(f"üîç GIBBERISH DETECTED: '{query}' ‚Üí s·ª≠ d·ª•ng context: '{prev_question}'")
                    
                    # T·∫°o hint ƒë·ªÉ LLM hi·ªÉu l√† follow-up
                    hint = f"[Follow-up question v·ªÅ: {prev_question}]"
                    return True, hint
                else:
                    return True, None
        
        return False, None

    def resolve_pronoun(self, user_input: str, history: List[dict] = None) -> Tuple[str, Optional[str]]:
        """
        üîß IMPROVED: X·ª≠ l√Ω ƒë·∫°i t·ª´ + gibberish + tr·∫£ v·ªÅ (rewritten_query, site_hint)
        - N·∫øu c√≥ ƒë·∫°i t·ª´ ‚Üí s·ª≠ d·ª•ng context l·ªãch s·ª≠
        - N·∫øu gibberish ‚Üí s·ª≠ d·ª•ng topic tr∆∞·ªõc
        - Tr·∫£ v·ªÅ site hint ƒë·ªÉ planner c√≥ th√™m info
        """
        pronouns = ["n√≥", "ƒë√≥", "ch·ªó n√†y", "n∆°i ƒë√≥", "ch·ªó ƒë√≥"]
        user_lower = user_input.lower()
        history = history or []
        
        # üîß B∆Ø·ªöC 1: Ki·ªÉm tra pronoun
        has_pronoun = any(p in user_lower for p in pronouns)
        
        if has_pronoun and history:
            # T√¨m c√¢u h·ªèi g·∫ßn nh·∫•t trong l·ªãch s·ª≠
            for entry in reversed(history[-3:]):  # Ki·ªÉm tra 3 c√¢u g·∫ßn nh·∫•t
                user_q = entry.get('user_input', '').lower()
                
                # T√¨m site t·ª´ c√¢u h·ªèi tr∆∞·ªõc
                prev_site = entry.get('site')
                if prev_site:
                    logger.info(f"üìå Pronoun resolved: '{user_input}' ‚Üí site={prev_site}")
                    return user_input, prev_site
        
        # üîß B∆Ø·ªöC 2: Ki·ªÉm tra gibberish
        is_gibberish, gibberish_hint = self.detect_gibberish_query(user_input, history)
        if is_gibberish:
            logger.info(f"üîç Gibberish query detected: '{user_input}'")
            if history:
                # L·∫•y topic + site t·ª´ c√¢u h·ªèi tr∆∞·ªõc
                prev_entry = history[-1]
                prev_question = prev_entry.get('user_input', '')
                prev_site = prev_entry.get('site')
                
                # T·∫°o rewritten query k·∫øt h·ª£p gibberish + context tr∆∞·ªõc
                rewritten = f"{user_input} (theo c√¢u h·ªèi tr∆∞·ªõc: {prev_question})"
                logger.info(f"   Rewritten: '{user_input}' ‚Üí site={prev_site}, hint={prev_question}")
                return rewritten, prev_site
        
        # Kh√¥ng c√≥ pronoun/gibberish ‚Üí tr·∫£ query g·ªëc + None
        return user_input, None



    async def query(self, collection_name: str, query_vector: List[float], limit: int, filter_dict: dict):
        """
        Truy v·∫•n Vector Database v√† tr·∫£ v·ªÅ c√°c k·∫øt qu·∫£ ph√π h·ª£p.
        """
        # S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c query t·ª´ VectorDatabase
        results = self.v_db.query(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            filter_dict=filter_dict
        )
        return results
