import json
import logging
import asyncio
from datetime import datetime
from zoneinfo import ZoneInfo
from app.services.knowledge import KnowledgeBase
from app.services.tools import HeritageTools
from app.core.config_prompts import get_planner_prompt, SEN_CHARACTER_PROMPT
from app.core.config_loader import get_default_site_key
# Vietnam timezone
VN_TZ = ZoneInfo('Asia/Ho_Chi_Minh')

# Kh·ªüi t·∫°o logger
logger = logging.getLogger("uvicorn")

async def agentic_workflow_stream(u_input: str, history: list, state, use_verifier: bool = False):
    """
    Unified Streaming Workflow for Agentic RAG.
    M·ªçi intent (Heritage, Chitchat, Realtime) ƒë·ªÅu tu√¢n th·ªß output stream chu·∫©n.
    Yields:
    - {"status": "processing", "step": N, "message": "..."}
    - {"status": "streaming", "content": "..."} (Text tokens)
    - {"status": "finished", "result": FinalJSON}
    """
    try:
        # [STEP 1] Normalize Input
        yield {"status": "processing", "step": 1, "message": "ƒêang ph√¢n t√≠ch c√¢u h·ªèi..."}
        norm_input = state.brain.normalize_query(u_input)
        
        # Build history string

        hist_str = ""
        for i, entry in enumerate(history[-6:]):
            if isinstance(entry, dict):
                # Support standard OpenAI format {role, content} from Node.js
                if 'role' in entry and 'content' in entry:
                    role = entry.get('role', '').capitalize()
                    content = entry.get('content', '')
                    # Truncate AI response ‚Äî rewrite ch·ªâ c·∫ßn bi·∫øt ng·ªØ c·∫£nh, kh√¥ng c·∫ßn full text
                    if role.lower() == 'assistant':
                        content = content[:200] + ('...' if len(content) > 200 else '')
                    hist_str += f"{role}: {content}\n"
                else:
                    # Support internal format {user_input, generated_answer}
                    q = entry.get('user_input', '')
                    a = entry.get('generated_answer', '')[:200]
                    if q: hist_str += f"User: {q}\n"
                    if a: hist_str += f"AI: {a}...\n"
            elif isinstance(entry, str):
                role = "User" if i % 2 == 0 else "AI"
                hist_str += f"{role}: {entry[:200]}\n"

        # [STEP 1] Contextualize Query (Rewrite if history exists)
        search_query = norm_input # M·∫∑c ƒë·ªãnh l√† input g·ªëc
        if hist_str.strip(): 
            yield {"status": "processing", "step": 1, "message": "Hi·ªÉu ng·ªØ c·∫£nh h·ªôi tho·∫°i..."}
            try:
                from app.core.config_prompts import get_contextualize_prompt
                rw_res = await state.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": get_contextualize_prompt()},
                        {"role": "user", "content": f"History:\n{hist_str}\n\nInput: {norm_input}"}
                    ]
                )
                rewrite = rw_res.choices[0].message.content.strip()
                # Ki·ªÉm tra sanity check: Kh√¥ng ƒë∆∞·ª£c qu√° ng·∫Øn ho·∫∑c l√† ch√≠nh input c≈©
                if rewrite and len(rewrite) > 4 and rewrite != norm_input:
                    logger.info(f"üîÑ [REWRITE] '{norm_input}' ‚Üí '{rewrite}'")
                    search_query = rewrite
            except Exception as e:
                logger.error(f"‚ùå Rewrite error: {e}")
        
        # --- REDIS CACHE CHECK (after query rewrite) ---
        # ‚≠ê KI·ªÇM TRA GAME CONTEXT (Level/Character/Knowledge Base)
        # N·∫øu c√≥ game context ‚Üí KH√îNG D√ôNG CACHE ƒë·ªÉ ƒë·∫£m b·∫£o ƒë√∫ng KB + persona
        has_game_context = False
        level_context_name = ""
        for entry in history:
            if isinstance(entry, dict) and entry.get('role') == 'system':
                content = entry.get('content', '')
                # Check n·∫øu c√≥ Level, Chapter, ho·∫∑c Knowledge Base trong system message
                if any(keyword in content for keyword in ['Level:', 'Chapter:', 'KI·∫æN TH·ª®C RI√äNG', 'üìç CONTEXT:', 'üìö']):
                    has_game_context = True
                    logger.info("üéÆ Game context detected ‚Üí Cache DISABLED")
                    
                    # Extract specifically the Level Name if available for Planner Constraint
                    import re
                    # Match pattern: - Level: "t√™n level"
                    match = re.search(r'- Level:\s*"(.*?)"', content)
                    if match:
                        level_context_name = match.group(1)
                        logger.info(f"   ‚Üí Explicit Level Context: '{level_context_name}'")
                    break
        
        # ‚≠ê SEMANTIC CACHE CHECK ‚Äî sau rewrite ƒë·ªÉ d√πng search_query ƒë√£ chu·∫©n h√≥a
        # VD: 'c·ªôt c·ªù ·ªü ƒë√≥' ‚Üí rewrite ‚Üí 'C·ªôt c·ªù Ho√†ng Th√†nh ThƒÉng Long' ‚Üí similarity=0.97 ‚Üí HIT!
        if not has_game_context and hasattr(state, 'sem_cache'):
            cached = state.sem_cache.get(search_query, intent_filter="heritage")
            if cached:
                yield {"status": "processing", "step": 1.1, "message": "ƒê√£ t√¨m th·∫•y trong b·ªô nh·ªõ..."}
                logger.info(f"‚úÖ [SemanticCache HIT] query='{search_query[:50]}'")
                full_text = cached.get("answer", "")
                for i in range(0, len(full_text), 10):
                    yield {"status": "streaming", "content": full_text[i:i+10]}
                    await asyncio.sleep(0.005)
                yield {"status": "finished", "result": cached}
                return

        # [STEP 1.5] Semantic Site Retrieval (Routing using Search Query)
        yield {"status": "processing", "step": 1.5, "message": "ƒêang ƒë·ªãnh tuy·∫øn ng·ªØ nghƒ©a..."}
        
        # T√¨m site ti·ªÅm nƒÉng ƒë·ªÉ planner quy·∫øt ƒë·ªãnh t·ªët h∆°n
        try:
            candidate_sites = state.brain.find_potential_sites(search_query, top_k=3)
        except Exception:
            candidate_sites = []

        dynamic_prompt = get_planner_prompt(candidate_sites, level_context=level_context_name)
        planner_input = f"History:\n{hist_str}\n\nOriginal Input: {norm_input}\nRewritten Input: {search_query}\n"

        # G·ªçi LLM Planner
        plan_res = await state.openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": dynamic_prompt},
                {"role": "user", "content": planner_input}
            ]
        )
        
        try:
            plan = json.loads(plan_res.choices[0].message.content)
            intent = plan.get("intent", "chitchat")
            site_key = plan.get("site")
        except:
            intent = "chitchat"
            site_key = None

        logger.info(f"üìã Intent: {intent} | Site: {site_key}")
        yield {"status": "processing", "step": 3, "message": f"√ù ƒë·ªãnh: {intent} ({site_key})"}

        # [STEP 3] Execution based on Intent
        final_context = ""
        source_type = "none"
        full_answer = ""  # Initialize to avoid UnboundLocalError

        # --- CASE A: OUT OF SCOPE ---

        if intent == "out_of_scope":
            # [IMPROVED] D√πng LLM ƒë·ªÉ t·ª´ ch·ªëi kh√©o l√©o theo Persona thay v√¨ hardcode
            yield {"status": "processing", "step": 3.1, "message": "Sen ƒëang suy nghƒ© c√¢u tr·∫£ l·ªùi..."}
            
            # L·∫•y danh s√°ch c√°c site ƒêANG C√ì trong h·ªá th·ªëng ƒë·ªÉ bot t·ª± tin gi·ªõi thi·ªáu
            from app.core.config_loader import get_heritage_config
            known_sites = get_heritage_config()
            known_sites_names = [s['name'] for s in known_sites.values()]
            known_list_str = ", ".join(known_sites_names)

            refusal_prompt = f"""{SEN_CHARACTER_PROMPT}
            
            NG∆Ø·ªúI D√ôNG V·ª™A H·ªéI V·ªÄ: "{u_input}"
            
            S·ª∞ TH·∫¨T L√Ä:
            1. B·∫°n KH√îNG c√≥ d·ªØ li·ªáu v·ªÅ ƒë·ªãa ƒëi·ªÉm/ch·ªß ƒë·ªÅ n√†y trong s·ªï tay.
            2. B·∫°n HI·ªÜN T·∫†I CH·ªà C√ì D·ªÆ LI·ªÜU V·ªÄ: {known_list_str}.
            
            NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
            - Nh·∫Øc l·∫°i t√™n ƒë·ªãa ƒëi·ªÉm ng∆∞·ªùi d√πng v·ª´a h·ªèi (ƒë·ªÉ x√°c nh·∫≠n l√† b·∫°n hi·ªÉu ƒë√∫ng √Ω).
            - Xin l·ªói kh√©o l√©o r·∫±ng hi·ªán t·∫°i trong s·ªï tay c·ªßa Sen ch∆∞a k·ªãp c·∫≠p nh·∫≠t th√¥ng tin v·ªÅ n∆°i ƒë√≥.
            - TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c b·ªãa ra c√°c ch·ªß ƒë·ªÅ li√™n quan (nh∆∞ m√≥n ƒÉn, l·ªÖ h·ªôi v√πng ƒë√≥) ƒë·ªÉ g·ª£i √Ω.
            - CH·ªà ƒê∆Ø·ª¢C M·ªúI ng∆∞·ªùi d√πng tham quan c√°c ƒë·ªãa ƒëi·ªÉm b·∫°n ƒêANG BI·∫æT ({known_list_str}).
            - Gi·ªØ th√°i ƒë·ªô c·∫ßu th·ªã, h·ª©a s·∫Ω h·ªçc th√™m trong t∆∞∆°ng lai.
            """
            
            try:
                res = await state.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "system", "content": refusal_prompt}],
                    temperature=0.7
                )
                response_msg = res.choices[0].message.content
            except Exception as e:
                logger.error(f"Out of scope generation error: {e}")
                response_msg = "D·∫°, Sen ch·ªâ ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ Di s·∫£n, VƒÉn h√≥a v√† L·ªãch s·ª≠ Vi·ªát Nam th√¥i ·∫°. B√°c vui l√≤ng h·ªèi ch·ªß ƒë·ªÅ li√™n quan ƒë·ªÉ Sen ph·ª•c v·ª• nh√©! üáªüá≥"

            yield {"status": "streaming", "content": response_msg}
            final_res = await _build_response_data(state, response_msg, intent, site_key, "none")
            yield {"status": "finished", "result": final_res}
            return

        # --- CASE B: CHITCHAT ---
        if intent == "chitchat":
            # Inject Current Time
            current_time = datetime.now(VN_TZ).strftime("%H:%M ng√†y %d/%m/%Y")
            
            # [FIX] Level Context Constraint for Chitchat Refusal
            level_refusal_prompt = ""
            if level_context_name:
                level_refusal_prompt = f"""
[GAMEPLAY MODE]: Ng∆∞·ªùi d√πng ƒëang ch∆°i m√†n: "{level_context_name}".
[CH·ªà D·∫™N QUAN TR·ªåNG]: 
- N·∫øu c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng l√† v·ªÅ m·ªôt DI T√çCH/S·ª∞ KI·ªÜN KH√ÅC (kh√¥ng ph·∫£i {level_context_name}), b·∫°n h√£y KH√âO L√âO T·ª™ CH·ªêI cung c·∫•p th√¥ng tin chi ti·∫øt. 
- H√£y g·ª£i √Ω quay l·∫°i ch·ªß ƒë·ªÅ c·ªßa m√†n ch∆°i hi·ªán t·∫°i.
- M·∫´u c√¢u: "D·∫°, c·∫≠u ∆°i, m√¨nh ƒëang ·ªü {level_context_name} m√†? C·ª© t·∫≠p trung v√†o ƒë√¢y ƒë√£ nh√©, chuy·ªán ƒë√≥ ƒë·ªÉ l√∫c kh√°c Sen k·ªÉ cho!"
- N·∫øu l√† ch√†o h·ªèi x√£ giao (hi, hello) -> Tr·∫£ l·ªùi b√¨nh th∆∞·ªùng.
"""

            # [FIX] Inject Candidate Sites into Chitchat Context for smarter clarification
            # N·∫øu Planner ƒë·∫©y v·ªÅ Chitchat v√¨ c√¢u h·ªèi ng·∫Øn (vd: "l√° c·ªù"), nh∆∞ng th·ª±c t·∫ø KnowledgeBase l·∫°i t√¨m th·∫•y "Ho√†ng Th√†nh"
            # th√¨ Bot n√™n d√πng th√¥ng tin ƒë√≥ ƒë·ªÉ g·ª£i √Ω ng∆∞·ª£c l·∫°i User.
            candidate_names = []
            if candidate_sites:
                candidate_names = [site['name'] for site in candidate_sites]
            
            site_hint_msg = ""
            if candidate_names:
                site_hint_msg = f"\n[G·ª¢I √ù T·ª™ H·ªÜ TH·ªêNG]: T·ª´ kh√≥a trong c√¢u h·ªèi c√≥ th·ªÉ li√™n quan ƒë·∫øn: {', '.join(candidate_names)}. H√£y d√πng th√¥ng tin n√†y ƒë·ªÉ h·ªèi l·∫°i ng∆∞·ªùi d√πng (V√≠ d·ª•: '√ù c·∫≠u l√† C·ªôt C·ªù ·ªü Ho√†ng Th√†nh ph·∫£i kh√¥ng?')."

            # [FIX] Inject Explicit Known Sites for Recommendation to prevent Hallucination
            from app.core.config_loader import get_heritage_config
            all_known = get_heritage_config()
            # Format list: "T√™n (M√¥ t·∫£ ng·∫Øn)"
            valid_recommendations = []
            for k, v in all_known.items():
                valid_recommendations.append(f"- {v['name']}")
            
            valid_recs_str = "\n".join(valid_recommendations)

            system_msg = f"""{SEN_CHARACTER_PROMPT}

[TH√îNG TIN TH·ªúI GIAN TH·ª∞C]: Hi·ªán t·∫°i l√† {current_time}.
{level_refusal_prompt}
{site_hint_msg}

[DANH S√ÅCH ƒê·ªäA ƒêI·ªÇM B·∫†N C√ì D·ªÆ LI·ªÜU]:
{valid_recs_str}

[QUY T·∫ÆC S·ªêNG C√íN - CH·∫∂N CH·ª¶ ƒê·ªÄ L·∫†]:
1. KI·ªÇM TRA CH·ª¶ ƒê·ªÄ C√ÇU N√ìI:
   - N·∫øu ng∆∞·ªùi d√πng KHEN/CH√ä/H·ªéI v·ªÅ m√≥n ƒÉn, ƒë·ªãa ƒëi·ªÉm, s·ª± v·∫≠t KH√îNG N·∫∞M TRONG DANH S√ÅCH: {valid_recs_str.replace(chr(10), ", ")}.
   - H√ÄNH ƒê·ªòNG: TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C H∆Ø·ªûNG ·ª®NG hay b√¨nh lu·∫≠n s√¢u (D√π l√† ƒë·ªìng t√¨nh "ngon l·∫Øm", "ƒë·∫πp l·∫Øm").
   - PH·∫¢I l√°i ngay v·ªÅ ch·ªß ƒë·ªÅ Sen bi·∫øt.
   - V√≠ d·ª• SAI: "B√°nh P√≠a ngon l·∫Øm! V·ªè gi√≤n tan..." (SAI v√¨ B√°nh P√≠a kh√¥ng c√≥ trong DB).
   - V√≠ d·ª• ƒê√öNG: "D·∫° nghe t·∫£ b√°nh P√≠a h·∫•p d·∫´n qu√°, nh∆∞ng ti·∫øc l√† Sen ch∆∞a ƒë∆∞·ª£c h·ªçc k·ªπ v·ªÅ ·∫©m th·ª±c v√πng n√†y. Hay l√† m√¨nh quay v·ªÅ b√†n chuy·ªán M√∫a R·ªëi N∆∞·ªõc ƒëi ·∫°? C≈©ng th√∫ v·ªã l·∫Øm ƒë√≥!"

2. TUY·ªÜT ƒê·ªêI KH√îNG D√ôNG KI·∫æN TH·ª®C GPT-4 (PRE-TRAINED KNOWLEDGE) ƒë·ªÉ m√¥ t·∫£ nh·ªØng th·ª© ngo√†i Database.
   - N·∫øu b·∫°n kh√¥ng c√≥ d·ªØ li·ªáu v·ªÅ "B√°nh P√≠a" trong monuments.json -> Coi nh∆∞ b·∫°n KH√îNG BI·∫æT n√≥ l√† g√¨.

3. KHI NG∆Ø·ªúI D√ôNG NH·ªú G·ª¢I √ù/RECOMMEND:
   - CH·ªà ƒê∆Ø·ª¢C G·ª¢I √ù c√°c ƒë·ªãa ƒëi·ªÉm trong danh s√°ch ·ªü tr√™n.
   
4. KH√îNG "D·∫†Y ƒê·ªúI" TRONG CHITCHAT:
   - Tuy·ªát ƒë·ªëi KH√îNG ƒë∆∞a ra c√°c th√¥ng tin l·ªãch s·ª≠ c·ª• th·ªÉ.
   - Thay v√†o ƒë√≥: H√£y gi·ªõi thi·ªáu c·∫£m x√∫c, kh√¥ng gian, v·∫ª ƒë·∫πp.

[QUY T·∫ÆC FORMAT]: N·∫øu c√≥ Link/URL, B·∫ÆT BU·ªòC ƒë·ªãnh d·∫°ng Markdown: [T√™n Link](URL). Kh√¥ng ƒë·ªÉ URL tr·∫ßn."""
            
            # [FIX] C·∫ßn truy·ªÅn to√†n b·ªô HISTORY ƒë·ªÉ LLM nh√¨n th·∫•y System Prompt (c√≥ ch·ª©a Knowledge Base t·ª´ AI Service)
            # L·ªçc history ƒë·ªÉ ch·ªâ l·∫•y ƒë√∫ng format OpenAI support (role, content)
            chat_messages = [{"role": "system", "content": system_msg}]
            
            for entry in history:
                if isinstance(entry, dict) and 'role' in entry and 'content' in entry:
                    chat_messages.append({"role": entry['role'], "content": entry['content']})
            
            # Append c√∫ ch√≥t l√† user input hi·ªán t·∫°i (n·∫øu ch∆∞a c√≥ trong history)
            if not chat_messages or chat_messages[-1]['role'] != 'user' or chat_messages[-1]['content'] != u_input:
                 chat_messages.append({"role": "user", "content": u_input})

            res = await state.openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=chat_messages,
                stream=True
            )
            full_ans = ""
            async for chunk in res:
                txt = chunk.choices[0].delta.content
                if txt:
                    full_ans += txt
                    yield {"status": "streaming", "content": txt}
            
            # Thay v√¨ return ngay, g√°n v√†o full_answer v√† ƒë·ªÉ code ch·∫°y ti·∫øp xu·ªëng ph·∫ßn TTS
            full_answer = full_ans
            final_context = "Chitchat conversation" # Dummy context for logging
            # yield {"status": "finished", "result": {"answer": full_ans, "intent": intent, "site": site_key}}
            # return

        # --- MAIN FLOW: HERITAGE & REALTIME ---
        # 1. Static Info
        from app.core.config_loader import get_site_config
        static_info = ""
        if site_key:
            site_config = get_site_config(site_key)
            if site_config:
                static_info = f"TH√îNG TIN DI T√çCH ({site_config.get('name')}):\n ƒê·ªãa ch·ªâ: {site_config.get('address')}\nGi·ªù m·ªü c·ª≠a: {site_config.get('open_hour')}h-{site_config.get('close_hour')}h"
        
        # 2. Dynamic Info / RAG
        # 2. Dynamic Info / RAG
        if intent == "realtime":
            if not site_key: 
                err = "D·∫° b√°c mu·ªën h·ªèi th√¥ng tin n√†y ·ªü ƒë·ªãa ƒëi·ªÉm n√†o ·∫°? (Ho√†ng Th√†nh, VƒÉn Mi·∫øu...)"
                yield {"status": "streaming", "content": err}
                yield {"status": "finished", "result": {"answer": err, "intent": intent, "site": None}}
                return
            
            yield {"status": "processing", "step": 4, "message": f"K·∫øt n·ªëi d·ªØ li·ªáu th·ª±c t·∫ø t·∫°i {site_key}..."}
            
            # Ki·ªÉm tra xem c√≥ c·∫ßn RAG kh√¥ng ‚Äî query thu·∫ßn realtime (gi√° v√©, th·ªùi ti·∫øt, gi·ªù) th√¨ skip RAG
            PURE_REALTIME_KEYWORDS = ["gi√° v√©", "v√© v√†o", "m·∫•y gi·ªù", "m·ªü c·ª≠a", "ƒë√≥ng c·ª≠a", "th·ªùi ti·∫øt", "m∆∞a", "n·∫Øng", "nhi·ªát ƒë·ªô"]
            check_text = norm_input + " " + search_query.lower()  # Ki·ªÉm tra c·∫£ c√¢u g·ªëc l·∫´n rewrite
            is_pure_realtime = any(kw in check_text for kw in PURE_REALTIME_KEYWORDS)
            
            if is_pure_realtime:
                logger.info(f"‚ö° [REALTIME ONLY] Skip RAG ‚Äî query thu·∫ßn realtime: '{norm_input[:40]}'")
                rag_context_str = ""
            else:
                # C√¢u h·ªèi realtime c√≥ k·∫øt h·ª£p l·ªãch s·ª≠/vƒÉn h√≥a ‚Üí c·∫ßn RAG context
                logger.info(f"üîç [REALTIME + RAG] Fetching base knowledge for {site_key}...")
                rag_content = await state.brain.fetch_and_rerank(
                    query=search_query,
                    site_key=site_key,
                    history=history
                )
                rag_context_str = f"\n\nTH√îNG TIN L·ªäCH S·ª¨/VƒÇN H√ìA:\n{rag_content}" if rag_content else ""

            try:
                # G·ªçi song song c√°c tool
                tasks = [
                    state.tools.get_weather(site_key),
                    state.tools.get_ticket_prices(site_key),
                    asyncio.to_thread(state.tools.get_opening_status, site_key)
                ]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Filter k·∫øt qu·∫£ h·ª£p l·ªá (kh√¥ng None, kh√¥ng Exception)
                valid_results = []
                for r in results:
                    if isinstance(r, Exception):
                        logger.error(f"Tool error: {r}")
                        continue
                    if r and isinstance(r, str) and len(r.strip()) > 0:
                        valid_results.append(r)
                
                if not valid_results:
                    # N·∫øu t·∫•t c·∫£ tools ƒë·ªÅu fail
                    realtime_data = "Hi·ªán t·∫°i Sen ch∆∞a k·∫øt n·ªëi ƒë∆∞·ª£c v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu th·ªùi gian th·ª±c."
                else:
                    realtime_data = "\n\n".join(valid_results)
                
                final_context = f"{static_info}{rag_context_str}\n\n{'='*50}\nD·ªÆ LI·ªÜU TH·ªúI GIAN TH·ª∞C:\n{'='*50}\n{realtime_data}"
                source_type = "tools+rag"
            except Exception as e:
                logger.error(f"Realtime Tool Error: {e}", exc_info=True)
                final_context = f"{static_info}{rag_context_str}\n\n(L·ªói k·∫øt n·ªëi c√¥ng c·ª• th·ªùi gian th·ª±c)"
                source_type = "rag_only_fallback"

        elif intent == "heritage": # RAG
            yield {"status": "processing", "step": 4, "message": "Tra c·ª©u s·ª≠ li·ªáu..."}
            
            # Log chi ti·∫øt Heritage routing
            logger.info(f"üîç [HERITAGE RAG] Raw: '{u_input}' -> Search: '{search_query}'")
            logger.info(f"   ‚Üí Site Key: '{site_key}' (Collection & Filter wil be loaded from config)")
            
            # L∆∞u √Ω: KnowledgeBase ƒë√£ c√≥ logic global fallback n√™n c·ª© g·ªçi
            rag_content = await state.brain.fetch_and_rerank(
                query=search_query, 
                site_key=site_key,
                history=history # [STATELESS] Pass user history
            )

            # [STRICT MODE] Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu RAG
            # fetch_and_rerank c√≥ th·ªÉ tr·∫£ v·ªÅ string r·ªóng ho·∫∑c None
            if not rag_content or not rag_content.strip():
                # ‚ú® STRICT MODE UPDATE: User y√™u c·∫ßu CH·ªà l·∫•y t·ª´ DB.
                # N·∫øu kh√¥ng c√≥, log l·ªói v√† b√°o kh√¥ng t√¨m th·∫•y.
                logger.error("‚ùå MONGODB RETRIEVAL FAILED: Kh√¥ng t√¨m th·∫•y document n√†o kh·ªõp c√¢u h·ªèi.")
                
                fallback_msg = "Xin l·ªói, Sen kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong C∆° s·ªü d·ªØ li·ªáu c·ªßa m√¨nh (MongoDB tr·∫£ v·ªÅ r·ªóng). Vui l√≤ng ki·ªÉm tra l·∫°i k·∫øt n·ªëi ho·∫∑c d·ªØ li·ªáu."
                yield {"status": "streaming", "content": fallback_msg}
                yield {"status": "finished", "result": {"answer": fallback_msg, "intent": intent, "site": site_key}}
                return

            final_context = f"{static_info}\n\nTH√îNG TIN L·ªäCH S·ª¨/VƒÇN H√ìA:\n{rag_content}"
            source_type = "rag"

        # --- CASE C: GENERATION (Only if answer not yet generated) ---
        if not full_answer:
            # [STEP 7] Generator (Stream) WITH MEMORY & REDIS CACHE
            yield {"status": "processing", "step": 5, "message": "Sen ƒëang tr·∫£ l·ªùi..."}
            
            # T√°i t·∫°o tin nh·∫Øn context (Memory Injection)
            current_time = datetime.now(VN_TZ).strftime("%H:%M ng√†y %d/%m/%Y")
            
            # [IMPROVED] FALLBACK MECHANISM FOR GENERAL KNOWLEDGE
            # N·∫øu RAG Context (final_context) qu√° ng·∫Øn ho·∫∑c r·ªóng, nh∆∞ng c√¢u h·ªèi l·∫°i v·ªÅ ki·∫øn th·ª©c ph·ªï th√¥ng (VD: "√Ω nghƒ©a sao v√†ng 5 c√°nh"),
            # cho ph√©p LLM "ch√©m" d·ª±a tr√™n ki·∫øn th·ª©c chung nh∆∞ng ph·∫£i ƒë√°nh d·∫•u l√† ki·∫øn th·ª©c b·ªï tr·ª£.
            
            allow_general_knowledge_instruction = ""
            if intent == "heritage" and (not final_context or len(final_context) < 50): # Check final_context, not rag_content directly
                 allow_general_knowledge_instruction = """
[CH·∫æ ƒê·ªò KI·∫æN TH·ª®C B·ªî TR·ª¢]:
Hi·ªán t·∫°i t√†i li·ªáu (Context) kh√¥ng c√≥ ƒë·ªß th√¥ng tin cho c√¢u h·ªèi n√†y.
TUY NHI√äN, ƒë√¢y l√† m·ªôt c√¢u h·ªèi v·ªÅ ki·∫øn th·ª©c ph·ªï th√¥ng/bi·ªÉu t∆∞·ª£ng vƒÉn h√≥a (V√≠ d·ª•: √ù nghƒ©a sao v√†ng, Ch√∫ T·ªÖu l√† ai...).
H√ÉY TR·∫¢ L·ªúI d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n, NH∆ØNG ph·∫£i b·∫Øt ƒë·∫ßu b·∫±ng c·ª•m t·ª´:
"Theo hi·ªÉu bi·∫øt chung c·ªßa Sen th√¨..." ho·∫∑c "Tuy trong s·ªï tay di t√≠ch kh√¥ng ghi r√µ, nh∆∞ng theo Sen bi·∫øt..."
TUY·ªÜT ƒê·ªêI KH√îNG B·ªäA ƒê·∫∂T c√°c th√¥ng tin c·ª• th·ªÉ nh∆∞ ng√†y th√°ng, s·ªë li·ªáu n·∫øu kh√¥ng ch·∫Øc ch·∫Øn.
"""

            system_prompt = f"""{SEN_CHARACTER_PROMPT}

[TH√îNG TIN TH·ªúI GIAN TH·ª∞C]: Hi·ªán t·∫°i l√† {current_time}.
[QUY T·∫ÆC FORMAT]: N·∫øu c√≥ Link/URL, B·∫ÆT BU·ªòC ƒë·ªãnh d·∫°ng Markdown: [T√™n Link](URL). Kh√¥ng ƒë·ªÉ URL tr·∫ßn.
{allow_general_knowledge_instruction}
[QUY T·∫ÆC G·ª¢I √ù]: Khi ng∆∞·ªùi d√πng nh·ªù gi·ªõi thi·ªáu/g·ª£i √Ω ƒë·ªãa ƒëi·ªÉm ƒëi ch∆°i, H√ÉY CH·ªà t·∫≠p trung v√†o c√°c DI S·∫¢N VƒÇN H√ìA, L·ªäCH S·ª¨, ho·∫∑c DANH LAM TH·∫ÆNG C·∫¢NH VI·ªÜT NAM (V√≠ d·ª•: Ho√†ng Th√†nh, VƒÉn Mi·∫øu, Ch√πa M·ªôt C·ªôt, Nh√† T√π H·ªèa L√≤...). Tr√°nh g·ª£i √Ω c√°c khu vui ch∆°i gi·∫£i tr√≠ thu·∫ßn t√∫y tr·ª´ khi ƒë∆∞·ª£c h·ªèi."""
            msgs = [{"role": "system", "content": system_prompt}]
            
            # Short history (Client sends full list but we take last 4 items)
            for entry in history[-4:]:
                 if isinstance(entry, dict):
                     # Standard OpenAI format
                     if 'role' in entry and 'content' in entry:
                         msgs.append({"role": entry['role'], "content": entry['content']})
                     # Internal format
                     else:
                         if 'user_input' in entry: msgs.append({"role": "user", "content": entry['user_input']})
                         if 'generated_answer' in entry: msgs.append({"role": "assistant", "content": entry['generated_answer']})
            
            # Prompt ch√≠nh
            user_p = f"TH√îNG TIN TRA C·ª®U (CONTEXT):\n{final_context}\n\nC√ÇU H·ªéI: {u_input}\n\nH√£y tr·∫£ l·ªùi d·ª±a tr√™n Context."
            msgs.append({"role": "user", "content": user_p})

            stream_resp = await state.openai.chat.completions.create(
                model="gpt-4o-mini", messages=msgs, stream=True
            )

            async for chunk in stream_resp:
                txt = chunk.choices[0].delta.content
                if txt:
                    full_answer += txt
                    yield {"status": "streaming", "content": txt}
        
        # [STEP 6: PREPARE DEBUG INFO]
        debug_col = "N/A"
        debug_filter = "N/A"
        
        if intent == "heritage" and site_key:
            from app.core.config_loader import get_site_config
            sc = get_site_config(site_key)
            if sc:
                 debug_col = sc.get("collection", "culture")
                 # Convert filter dict to string for display
                 flt = sc.get("filter", {})
                 debug_filter = json.dumps(flt, ensure_ascii=False) if flt else "Global Search"
        
        # [STEP 8] Optional Verifier
        # Ch·ªâ ch·∫°y n·∫øu b·∫≠t mode Verifier v√† c√≥ context ƒë·ªÉ ƒë·ªëi chi·∫øu (Heritage mode)
        if use_verifier and intent == "heritage" and 'final_context' in locals():
             yield {"status": "processing", "step": 5.5, "message": "üïµÔ∏è ƒêang ki·ªÉm ch·ª©ng th√¥ng tin..."}
             try:
                 from app.services.verifier import Verifier
                 # Init Verifier (ƒë·∫£m b·∫£o imports kh√¥ng l·ªói)
                 verifier = Verifier(state.openai)
                 
                 # Th·ª±c hi·ªán verify
                 verify_res = await verifier.verify(u_input, final_context, full_answer)
                 
                 note = ""
                 if verify_res.get("is_valid"):
                     note = f"\n\n----------\n‚úÖ [Ki·ªÉm ch·ª©ng]: {verify_res.get('reason')}"
                 else:
                     note = f"\n\n----------\n‚ö†Ô∏è [C·∫£nh b√°o]: {verify_res.get('reason')}"
                 
                 # Stream k·∫øt qu·∫£ ki·ªÉm ch·ª©ng ra giao di·ªán
                 yield {"status": "streaming", "content": note}
                 full_answer += note
                 
             except Exception as e:
                 logger.error(f"Verifier Error: {e}")

        # [AUTO TTS]
        audio_b64 = ""
        try:
            if hasattr(state, 'generate_tts') and full_answer:
                 yield {"status": "processing", "step": 6, "message": "ƒêang t·∫°o gi·ªçng ƒë·ªçc..."}
                 audio_b64 = await state.generate_tts(full_answer)
        except Exception as e:
            logger.error(f"‚ùå Auto TTS Failed: {e}")

        # [EMOTION ANALYSIS] üé≠ Ph√¢n t√≠ch bi·ªÉu c·∫£m
        emotion_data = {}
        try:
            from app.services.emotion import EmotionAnalyzer
            emotion_data = EmotionAnalyzer.analyze(u_input, full_answer, intent)
            logger.info(f"üé≠ Emotion selected: {emotion_data}")
        except Exception as e:
            logger.error(f"‚ùå Emotion Analysis failed: {e}")
            emotion_data = {"gesture": "normal", "mouthState": "smile", "eyeState": "normal"}

        # [FINISH]
        final_res = await _build_response_data(state, full_answer, intent, site_key, source_type, debug_col, debug_filter)
        final_res["audio_base64"] = audio_b64
        final_res["emotion"] = emotion_data  # ‚ú® Th√™m emotion metadata
        
        # ‚≠ê L∆∞u Semantic Cache SAU KHI ho√†n th√†nh ‚Äî key = search_query (ƒë√£ rewrite)
        if intent == "heritage" and not has_game_context and hasattr(state, 'sem_cache'):
            try:
                state.sem_cache.set(search_query, final_res, intent="heritage", ttl=3600)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [SemanticCache] Save error: {e}")


        yield {"status": "finished", "result": final_res}

    except Exception as e:
        logger.error(f"‚ùå Workflow Error: {e}", exc_info=True)
        err_msg = "√îi h·ªèng, Sen b·ªã v·∫•p c·ª•c ƒë√° (L·ªói h·ªá th·ªëng). B√°c h·ªèi l·∫°i gi√πm Sen nh√©!"
        yield {"status": "streaming", "content": err_msg}
        yield {"status": "finished", "result": {"answer": err_msg, "intent": "error", "site": None}}

async def _build_response_data(state, text, intent, site, source, collection="N/A", filter_info="N/A"):
    """Helper ƒë√≥ng g√≥i k·∫øt qu·∫£ cu·ªëi c√πng (ƒë·ªÉ cache ho·∫∑c debug)"""
    # Kh√¥ng c·∫ßn sinh audio base64 ·ªü ƒë√¢y n·ªØa v√¨ Frontend ƒë√£ t·ª± queue
    return {
        "answer": text,
        "intent": intent,
        "site": site,
        "data_source": source,
        "collection": collection,
        "filter": filter_info
    }

# Wrapper cho code c≈© n·∫øu c·∫ßn (nh∆∞ng n√™n d√πng stream)
async def agentic_workflow(u_input, history, state):
    res = None
    async for event in agentic_workflow_stream(u_input, history, state):
        if event["status"] == "finished":
            res = event["result"]
    return res