import json
import logging
import asyncio
from knowledge_base import KnowledgeBase
from heritage_tool import HeritageTools
from prompts import get_planner_prompt, SEN_CHARACTER_PROMPT
from data_manager import get_default_site_key

# Kh·ªüi t·∫°o logger
logger = logging.getLogger("uvicorn")

async def agentic_workflow_stream(u_input: str, history: list, state, use_verifier: bool = False):
    """
    Unified Streaming Workflow for Agentic RAG.
    M·ªçi intent (Heritage, Chitchat, Realtime) ƒë·ªÅu tu√¢n th·ªß output stream chu·∫©n.
    Yields:
    - {"status": "processing", "step": N, "message": "..."}
    - {"status": "streaming", "content": "..."} (Text tokens)
    - {"status": "finished", "result": FinalJSON}
    """
    try:
        # [STEP 1] Normalize Input
        yield {"status": "processing", "step": 1, "message": "ƒêang ph√¢n t√≠ch c√¢u h·ªèi..."}
        norm_input = state.brain.normalize_query(u_input)
        
        # --- REDIS CACHE CHECK (Only for consistent queries, can optimize later) ---
        # Chi·∫øn l∆∞·ª£c: Ki·ªÉm tra cache tr∆∞·ªõc. N·∫øu hit cache HERITAGE -> Stream gi·∫£ l·∫≠p t·ª´ cache.
        # Realtime kh√¥ng d√πng cache n√†y ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°i m·ªõi.
        cache_key = f"sen:cache:{norm_input}"
        cached_data = None
        try:
            if state.redis:
                cached_json = await state.redis.get(cache_key)
                if cached_json:
                    data = json.loads(cached_json)
                    # CH·ªà D√ôNG CACHE N·∫æU L√Ä HERITAGE
                    if data.get("intent") == "heritage":
                        cached_data = data
        except Exception as e:
            logger.warning(f"Redis Check Error: {e}")

        if cached_data:
            yield {"status": "processing", "step": 1.1, "message": "ƒê√£ t√¨m th·∫•y c√¢u tr·∫£ l·ªùi trong b·ªô nh·ªõ (Cache)..."}
            logger.info(f"‚úÖ Cache Hit: {cache_key}")
            
            # Stream gi·∫£ l·∫≠p t·ª´ text c√≥ s·∫µn
            full_text = cached_data.get("answer", "")
            chunk_size = 10
            for i in range(0, len(full_text), chunk_size):
                yield {"status": "streaming", "content": full_text[i:i+chunk_size]}
                await asyncio.sleep(0.01) # Gi·∫£ l·∫≠p ƒë·ªô tr·ªÖ t√≠ x√≠u cho m∆∞·ª£t
                
            yield {"status": "finished", "result": cached_data}
            return

        # Build history string
        hist_str = ""
        for i, entry in enumerate(history[-6:]):
            if isinstance(entry, dict):
                q = entry.get('user_input', '')
                a = entry.get('generated_answer', '')[:100]
                if q: hist_str += f"User: {q}\n"
                if a: hist_str += f"AI: {a}\n"
            elif isinstance(entry, str):
                role = "User" if i % 2 == 0 else "AI"
                hist_str += f"{role}: {entry[:200]}\n"

        # [STEP 1] Contextualize Query (Rewrite if history exists)
        search_query = norm_input # M·∫∑c ƒë·ªãnh l√† input g·ªëc
        if hist_str.strip(): 
            yield {"status": "processing", "step": 1, "message": "Hi·ªÉu ng·ªØ c·∫£nh h·ªôi tho·∫°i..."}
            try:
                from prompts import get_contextualize_prompt
                rw_res = await state.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": get_contextualize_prompt()},
                        {"role": "user", "content": f"History:\n{hist_str}\n\nInput: {norm_input}"}
                    ]
                )
                rewrite = rw_res.choices[0].message.content.strip()
                # Ki·ªÉm tra sanity check: Kh√¥ng ƒë∆∞·ª£c qu√° ng·∫Øn ho·∫∑c l√† ch√≠nh input c≈©
                if rewrite and len(rewrite) > 4 and rewrite != norm_input:
                    logger.info(f"üîÑ [REWRITE] '{norm_input}' ‚Üí '{rewrite}'")
                    search_query = rewrite
            except Exception as e:
                logger.error(f"‚ùå Rewrite error: {e}")

        # [STEP 1.5] Semantic Site Retrieval (Routing using Search Query)
        yield {"status": "processing", "step": 1.5, "message": "ƒêang ƒë·ªãnh tuy·∫øn ng·ªØ nghƒ©a..."}
        
        # T√¨m site ti·ªÅm nƒÉng ƒë·ªÉ planner quy·∫øt ƒë·ªãnh t·ªët h∆°n
        try:
            candidate_sites = state.brain.find_potential_sites(search_query, top_k=3)
        except Exception:
            candidate_sites = []

        dynamic_prompt = get_planner_prompt(candidate_sites)
        planner_input = f"History:\n{hist_str}\n\nOriginal Input: {norm_input}\nRewritten Input: {search_query}\n"

        # G·ªçi LLM Planner
        plan_res = await state.openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": dynamic_prompt},
                {"role": "user", "content": planner_input}
            ]
        )
        
        try:
            plan = json.loads(plan_res.choices[0].message.content)
            intent = plan.get("intent", "chitchat")
            site_key = plan.get("site")
        except:
            intent = "chitchat"
            site_key = None

        logger.info(f"üìã Intent: {intent} | Site: {site_key}")
        yield {"status": "processing", "step": 3, "message": f"√ù ƒë·ªãnh: {intent} ({site_key})"}

        # [STEP 3] Execution based on Intent
        final_context = ""
        source_type = "none"
        full_answer = ""  # Initialize to avoid UnboundLocalError

        # --- CASE A: OUT OF SCOPE ---

        if intent == "out_of_scope":
            response_msg = "D·∫°, Sen ch·ªâ ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ Di s·∫£n, VƒÉn h√≥a v√† L·ªãch s·ª≠ Vi·ªát Nam th√¥i ·∫°. B√°c vui l√≤ng h·ªèi ch·ªß ƒë·ªÅ li√™n quan ƒë·ªÉ Sen ph·ª•c v·ª• nh√©! üáªüá≥"
            # Stream response n√†y gi·∫£ l·∫≠p
            yield {"status": "streaming", "content": response_msg}
            final_res = await _build_response_data(state, response_msg, intent, site_key, "none")
            yield {"status": "finished", "result": final_res}
            return

        # --- CASE B: CHITCHAT ---
        if intent == "chitchat":
            res = await state.openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "system", "content": SEN_CHARACTER_PROMPT}, {"role": "user", "content": u_input}],
                stream=True
            )
            full_ans = ""
            async for chunk in res:
                txt = chunk.choices[0].delta.content
                if txt:
                    full_ans += txt
                    yield {"status": "streaming", "content": txt}
            
            # Thay v√¨ return ngay, g√°n v√†o full_answer v√† ƒë·ªÉ code ch·∫°y ti·∫øp xu·ªëng ph·∫ßn TTS
            full_answer = full_ans
            final_context = "Chitchat conversation" # Dummy context for logging
            # yield {"status": "finished", "result": {"answer": full_ans, "intent": intent, "site": site_key}}
            # return

        # --- MAIN FLOW: HERITAGE & REALTIME ---
        # 1. Static Info
        from data_manager import get_site_config
        static_info = ""
        if site_key:
            site_config = get_site_config(site_key)
            if site_config:
                static_info = f"TH√îNG TIN DI T√çCH ({site_config.get('name')}):\n ƒê·ªãa ch·ªâ: {site_config.get('address')}\nGi·ªù m·ªü c·ª≠a: {site_config.get('open_hour')}h-{site_config.get('close_hour')}h"
        
        # 2. Dynamic Info / RAG
        if intent == "realtime":
            if not site_key: 
                err = "D·∫° b√°c mu·ªën h·ªèi th√¥ng tin n√†y ·ªü ƒë·ªãa ƒëi·ªÉm n√†o ·∫°? (Ho√†ng Th√†nh, VƒÉn Mi·∫øu...)"
                yield {"status": "streaming", "content": err}
                yield {"status": "finished", "result": {"answer": err, "intent": intent, "site": None}}
                return
            
            yield {"status": "processing", "step": 4, "message": f"K·∫øt n·ªëi d·ªØ li·ªáu th·ª±c t·∫ø t·∫°i {site_key}..."}
            try:
                # G·ªçi song song c√°c tool
                tasks = [
                    state.tools.get_weather(site_key),
                    state.tools.get_ticket_prices(site_key),
                    asyncio.to_thread(state.tools.get_opening_status, site_key)
                ]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Filter k·∫øt qu·∫£ h·ª£p l·ªá (kh√¥ng None, kh√¥ng Exception)
                valid_results = []
                for r in results:
                    if isinstance(r, Exception):
                        logger.error(f"Tool error: {r}")
                        continue
                    if r and isinstance(r, str) and len(r.strip()) > 0:
                        valid_results.append(r)
                
                if not valid_results:
                    # N·∫øu t·∫•t c·∫£ tools ƒë·ªÅu fail
                    realtime_data = "Hi·ªán t·∫°i Sen ch∆∞a k·∫øt n·ªëi ƒë∆∞·ª£c v·ªõi c√°c ngu·ªìn d·ªØ li·ªáu th·ªùi gian th·ª±c. Xin l·ªói b√°c!"
                else:
                    realtime_data = "\n\n".join(valid_results)
                
                final_context = f"{static_info}\n\n{'='*50}\nD·ªÆ LI·ªÜU TH·ªúI GIAN TH·ª∞C:\n{'='*50}\n{realtime_data}"
                source_type = "tools"
            except Exception as e:
                logger.error(f"Realtime Tool Error: {e}", exc_info=True)
                final_context = static_info + "\n\n(L·ªói k·∫øt n·ªëi c√¥ng c·ª• th·ªùi gian th·ª±c)"
                source_type = "tools_error"

        elif intent == "heritage": # RAG
            yield {"status": "processing", "step": 4, "message": "Tra c·ª©u s·ª≠ li·ªáu..."}
            
            # Log chi ti·∫øt Heritage routing
            logger.info(f"üîç [HERITAGE RAG] Raw: '{u_input}' -> Search: '{search_query}'")
            logger.info(f"   ‚Üí Site Key: '{site_key}' (Collection & Filter wil be loaded from config)")
            
            # L∆∞u √Ω: KnowledgeBase ƒë√£ c√≥ logic global fallback n√™n c·ª© g·ªçi
            rag_content = await state.brain.fetch_and_rerank(
                query=search_query, 
                site_key=site_key,
                history=history # [STATELESS] Pass user history
            )

            # [STRICT MODE] Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu RAG
            # fetch_and_rerank c√≥ th·ªÉ tr·∫£ v·ªÅ string r·ªóng ho·∫∑c None
            if not rag_content or not rag_content.strip():
                fallback_msg = "D·∫°, hi·ªán t·∫°i trong th∆∞ vi·ªán c·ªßa Sen ch∆∞a c√≥ t√†i li·ªáu n√†o v·ªÅ ƒë·ªãa ƒëi·ªÉm n√†y ·∫° (Ho·∫∑c d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c n·∫°p). B√°c th√¥ng c·∫£m h·ªèi ƒë·ªãa ƒëi·ªÉm kh√°c nh√©!"
                yield {"status": "streaming", "content": fallback_msg}
                # K·∫øt th√∫c flow ngay, kh√¥ng cho LLM ch√©m gi√≥
                yield {"status": "finished", "result": {"answer": fallback_msg, "intent": intent, "site": site_key}}
                return

            final_context = f"{static_info}\n\nTH√îNG TIN L·ªäCH S·ª¨/VƒÇN H√ìA:\n{rag_content}"
            source_type = "rag"

        # --- CASE C: GENERATION (Only if answer not yet generated) ---
        if not full_answer:
            # [STEP 7] Generator (Stream) WITH MEMORY & REDIS CACHE
            yield {"status": "processing", "step": 5, "message": "Sen ƒëang tr·∫£ l·ªùi..."}
            
            # T√°i t·∫°o tin nh·∫Øn context (Memory Injection)
            system_prompt = SEN_CHARACTER_PROMPT
            msgs = [{"role": "system", "content": system_prompt}]
            
            # Short history (Client sends full list but we take last 4 items)
            for entry in history[-4:]:
                 if isinstance(entry, dict):
                     if 'user_input' in entry: msgs.append({"role": "user", "content": entry['user_input']})
                     if 'generated_answer' in entry: msgs.append({"role": "assistant", "content": entry['generated_answer']})
            
            # Prompt ch√≠nh
            user_p = f"TH√îNG TIN TRA C·ª®U (CONTEXT):\n{final_context}\n\nC√ÇU H·ªéI: {u_input}\n\nH√£y tr·∫£ l·ªùi d·ª±a tr√™n Context."
            msgs.append({"role": "user", "content": user_p})

            stream_resp = await state.openai.chat.completions.create(
                model="gpt-4o-mini", messages=msgs, stream=True
            )

            async for chunk in stream_resp:
                txt = chunk.choices[0].delta.content
                if txt:
                    full_answer += txt
                    yield {"status": "streaming", "content": txt}
        
        # [STEP 6: PREPARE DEBUG INFO]
        debug_col = "N/A"
        debug_filter = "N/A"
        
        if intent == "heritage" and site_key:
            from data_manager import get_site_config
            sc = get_site_config(site_key)
            if sc:
                 debug_col = sc.get("collection", "culture")
                 # Convert filter dict to string for display
                 flt = sc.get("filter", {})
                 debug_filter = json.dumps(flt, ensure_ascii=False) if flt else "Global Search"
        
        # [STEP 8] Optional Verifier
        # Ch·ªâ ch·∫°y n·∫øu b·∫≠t mode Verifier v√† c√≥ context ƒë·ªÉ ƒë·ªëi chi·∫øu (Heritage mode)
        if use_verifier and intent == "heritage" and 'final_context' in locals():
             yield {"status": "processing", "step": 5.5, "message": "üïµÔ∏è ƒêang ki·ªÉm ch·ª©ng th√¥ng tin..."}
             try:
                 from verifier import Verifier
                 # Init Verifier (ƒë·∫£m b·∫£o imports kh√¥ng l·ªói)
                 verifier = Verifier(state.openai)
                 
                 # Th·ª±c hi·ªán verify
                 verify_res = await verifier.verify(u_input, final_context, full_answer)
                 
                 note = ""
                 if verify_res.get("is_valid"):
                     note = f"\n\n----------\n‚úÖ [Ki·ªÉm ch·ª©ng]: {verify_res.get('reason')}"
                 else:
                     note = f"\n\n----------\n‚ö†Ô∏è [C·∫£nh b√°o]: {verify_res.get('reason')}"
                 
                 # Stream k·∫øt qu·∫£ ki·ªÉm ch·ª©ng ra giao di·ªán
                 yield {"status": "streaming", "content": note}
                 full_answer += note
                 
             except Exception as e:
                 logger.error(f"Verifier Error: {e}")

        # [AUTO TTS]
        audio_b64 = ""
        try:
            if hasattr(state, 'generate_tts') and full_answer:
                 yield {"status": "processing", "step": 6, "message": "ƒêang t·∫°o gi·ªçng ƒë·ªçc..."}
                 audio_b64 = await state.generate_tts(full_answer)
        except Exception as e:
            logger.error(f"‚ùå Auto TTS Failed: {e}")

        # [FINISH]
        final_res = await _build_response_data(state, full_answer, intent, site_key, source_type, debug_col, debug_filter)
        final_res["audio"] = audio_b64
        
        # --- REDIS SET (SAVE CACHE) ---
        # CH·ªà L∆ØU N·∫æU L√Ä HERITAGE
        if intent == "heritage" and state.redis:
            try:
                 # L∆∞u cache 1 ti·∫øng (3600s)
                 await state.redis.setex(cache_key, 3600, json.dumps(final_res, ensure_ascii=False))
                 logger.info(f"üíæ Caching HERITAGE response: {cache_key}")
            except Exception as e:
                 logger.warning(f"Redis Set Error: {e}")

        yield {"status": "finished", "result": final_res}

    except Exception as e:
        logger.error(f"‚ùå Workflow Error: {e}", exc_info=True)
        err_msg = "√îi h·ªèng, Sen b·ªã v·∫•p c·ª•c ƒë√° (L·ªói h·ªá th·ªëng). B√°c h·ªèi l·∫°i gi√πm Sen nh√©!"
        yield {"status": "streaming", "content": err_msg}
        yield {"status": "finished", "result": {"answer": err_msg, "intent": "error", "site": None}}

async def _build_response_data(state, text, intent, site, source, collection="N/A", filter_info="N/A"):
    """Helper ƒë√≥ng g√≥i k·∫øt qu·∫£ cu·ªëi c√πng (ƒë·ªÉ cache ho·∫∑c debug)"""
    # Kh√¥ng c·∫ßn sinh audio base64 ·ªü ƒë√¢y n·ªØa v√¨ Frontend ƒë√£ t·ª± queue
    return {
        "answer": text,
        "intent": intent,
        "site": site,
        "data_source": source,
        "collection": collection,
        "filter": filter_info
    }

# Wrapper cho code c≈© n·∫øu c·∫ßn (nh∆∞ng n√™n d√πng stream)
async def agentic_workflow(u_input, history, state):
    res = None
    async for event in agentic_workflow_stream(u_input, history, state):
        if event["status"] == "finished":
            res = event["result"]
    return res
