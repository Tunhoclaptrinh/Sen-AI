import os
import base64
import re
import json
import logging
from typing import List, Dict, Optional
from contextlib import asynccontextmanager
import time
import sys
import socket
import asyncio
from urllib.parse import parse_qs

from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import AsyncOpenAI 
import redis.asyncio as redis 
from edge_tts import Communicate
from langdetect import detect
from sentence_transformers import SentenceTransformer

# C√°c b·ªô chia nh·ªè vƒÉn b·∫£n (C·∫ßn thi·∫øt cho Ingest)
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# Module t√πy ch·ªânh c·ªßa Hi·∫øu
from vector_db import VectorDatabase
from reflection import Reflection
from my_semantic_logic.route import Route
from my_semantic_logic.router import SemanticRouter
import my_semantic_logic.samples as samples

# ====== 1. C·∫§U H√åNH GLOBAL ======
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DB_NAME = "vector_db"
COLLECTION_NAME = "culture"
REDIS_URL = os.getenv("REDIS_URL")

# Model 384 chi·ªÅu d√πng cho c·∫£ Router v√† Search
local_embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu
CULTURE_FILES = [
    ("mua_roi_nuoc", "mua_roi_nuoc.md"),
    ("hoang_thanh", "hoang_thanh.md"),
]

# ====== 2. H√ÄM H·ªñ TR·ª¢ N·∫†P D·ªÆ LI·ªÜU & LOGIC HYBRID ======

def simple_keyword_score(content: str, rewritten_query: str) -> float:
    """
    [M·ªöI B·ªî SUNG] 
    T√≠nh ƒëi·ªÉm th∆∞·ªüng d·ª±a tr√™n m·ª©c ƒë·ªô tr√πng l·∫∑p t·ª´ v·ª±ng gi·ªØa c√¢u h·ªèi v√† n·ªôi dung.
    """
    content_lower = content.lower()
    # T√°ch t·ª´, b·ªè qua c√°c t·ª´ qu√° ng·∫Øn (nh∆∞ 'l√†', '·ªü', 'c√≥')
    query_words = [w for w in rewritten_query.lower().split() if len(w) > 2]
    
    if not query_words:
        return 0.0
    
    matched_count = 0
    for word in query_words:
        if word in content_lower:
            matched_count += 1
            
    return matched_count / len(query_words)

def chunk_markdown(md_text: str) -> List[Dict]:
    headers_to_split_on = [("#", "h1"), ("##", "h2"), ("###", "h3")]
    splitter1 = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    sections = splitter1.split_text(md_text)
    splitter2 = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=180)
    docs = splitter2.split_documents(sections)
    return [{"content": d.page_content.strip(), "metadata": d.metadata} for d in docs if d.page_content.strip()]

def auto_ingest_data(v_db: VectorDatabase):
    """Ki·ªÉm tra v√† t·ª± ƒë·ªông n·∫°p d·ªØ li·ªáu 384 chi·ªÅu n·∫øu DB tr·ªëng"""
    for culture_type, file_path in CULTURE_FILES:
        if v_db.count_documents(COLLECTION_NAME, {"culture_type": culture_type}) == 0:
            if os.path.exists(file_path):
                logger.info(f"üîÑ ƒêang n·∫°p l·∫°i d·ªØ li·ªáu 384 chi·ªÅu cho: {culture_type}")
                with open(file_path, "r", encoding="utf-8") as f:
                    md_content = f.read()
                
                chunks = chunk_markdown(md_content)
                vectors = local_embedder.encode([c["content"] for c in chunks]).tolist()
                
                docs_to_insert = [
                    {
                        "content": c["content"],
                        "embedding": vectors[i],
                        "culture_type": culture_type,
                        "metadata": c["metadata"]
                    } for i, c in enumerate(chunks)
                ]
                v_db.insert_many(COLLECTION_NAME, docs_to_insert)
                logger.info(f"‚úÖ ƒê√£ n·∫°p xong {len(docs_to_insert)} ƒëo·∫°n cho {culture_type}")
            else:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {file_path}")

# ====== 3. ROUTER ======
routes = [
    Route(name="roi_nuoc", samples=samples.roiNuocSample, filter_dict={"culture_type": "mua_roi_nuoc"}),
    Route(name="hoang_thanh", samples=samples.hoangThanhSample, filter_dict={"culture_type": "hoang_thanh"}),
    Route(name="chitchat", samples=samples.chitchatSample, filter_dict={}),
]
router = SemanticRouter(embedding=local_embedder, routes=routes, threshold=0.5)

# ====== 4. MODELS ======
class ChatRequest(BaseModel):
    user_input: str
    history: List[Dict[str, str]]

class ChatResponse(BaseModel):
    answer: str
    rewritten_query: str
    route: str
    score: float
    audio_base64: Optional[str] = None
    context_used: Optional[str] = None

# ====== 5. LIFESPAN (Include Startup Logic Here) ======

def get_network_ip():
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except:
        return "localhost"

@asynccontextmanager
async def lifespan(app: FastAPI):
    # --- STARTUP LOGIC ---
    if not REDIS_URL:
        raise ValueError("‚ùå L·ªói: REDIS_URL ch∆∞a ƒë∆∞·ª£c khai b√°o trong .env")
        
    app.state.redis = redis.from_url(REDIS_URL, decode_responses=True)
    app.state.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    app.state.reflector = Reflection(llm_client=app.state.openai_client)
    
    # auto_ingest_data(vector_db) # User has this commented out in their snippet
    
    # Print Banner
    port = 8000
    network_ip = get_network_ip()
    env = os.getenv("ENV", "development")
    
    print(f"\n‚úÖ Server restart triggered at {time.strftime('%Y-%m-%dT%H:%M:%S.000Z')}", flush=True)
    
    import unicodedata

    def get_visual_width(s):
        width = 0
        for char in s:
            # Explicitly handle Emoji ranges and specific symbols that render wide
            code = ord(char)
            if (0x1F000 <= code <= 0x1F9FF) or (code == 0x2764) or (code == 0x2699):
                width += 2
                continue

            # Standard East Asian Width
            if unicodedata.east_asian_width(char) in ('W', 'F'):
                width += 2
            else:
                width += 1
        return width

    def print_line(icon, label, value, width=62):
        # Format: "   ICON  Label....... Value"
        label_part = f"{label:<13}"
        text = f"   {icon}  {label_part} {value}"
        
        # Robustly calculate exact visual display width
        vis_len = get_visual_width(text)
        
        padding = width - vis_len
        if padding < 0: padding = 0
        
        print(f"‚ïë{text}{' ' * padding}‚ïë", flush=True)

    border_width = 62
    border = "‚ïê" * border_width
    
    print(f"‚ïî{border}‚ïó", flush=True)
    
    # Center Title
    title_text = "üèõÔ∏è   Sen Server Started!"
    title_vis_len = get_visual_width(title_text)
    total_pad = border_width - title_vis_len
    left_pad = total_pad // 2
    right_pad = total_pad - left_pad
    print(f"‚ïë{' ' * left_pad}{title_text}{' ' * right_pad}‚ïë", flush=True)
    
    print(f"‚ï†{border}‚ï£", flush=True)
    print_line("üìç", "Local:", f"http://localhost:{port}", border_width)
    print_line("üì°", "Network:", f"http://{network_ip}:{port}", border_width)
    print_line("üåç", "Env:", f"{env}", border_width)
    print(f"‚ï†{border}‚ï£", flush=True)
    print_line("üìä", "API Docs:", f"http://localhost:{port}/docs", border_width)
    print_line("‚ù§Ô∏è", "Health:", f"http://localhost:{port}/", border_width)
    print(f"‚ïö{border}‚ïù", flush=True)

    yield
    
    # --- SHUTDOWN LOGIC ---
    await app.state.redis.close()

app = FastAPI(title="Heritage AI API", lifespan=lifespan)
vector_db = VectorDatabase(db_name=DB_NAME)

# ====== 6. UTILS ======
async def generate_audio_async(text: str) -> str:
    try:
        clean_text = re.sub(r'[*_#]', '', text).strip()
        lang = "vi-VN-HoaiMyNeural"
        try:
            if detect(clean_text[:30]) == 'en': lang = "en-US-GuyNeural"
        except: pass
        communicate = Communicate(clean_text, lang)
        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio": audio_data += chunk["data"]
        return base64.b64encode(audio_data).decode()
    except: return ""

# ====== MIDDLEWARE LOGGING (PURE ASGI - ROBUST) ======

def console_log(*args):
    print(*args, flush=True)

class ASGILoggerMiddleware:
    def __init__(self, app):
        self.app = app

    async def __call__(self, scope, receive, send):
        if scope["type"] not in ("http", "https"):
            return await self.app(scope, receive, send)

        # 1. Setup Request Logging
        start_time = time.time()
        method = scope["method"]
        path = scope["path"]
        query_string = scope.get("query_string", b"").decode()
        
        # 2. Log Request (NO BODY, as requested)
        console_log(f"\nüì• REQUEST ‚Üí {method} {path}")
        if query_string:
            try:
                parsed = parse_qs(query_string)
                clean_params = {k: v[0] if len(v) == 1 else v for k, v in parsed.items()}
                console_log("   Query:", clean_params)
            except:
                console_log("   Query:", query_string)
        
        # 3. Wrap Send to capture Response
        async def wrapped_send(message):
            if message["type"] == "http.response.start":
                status_code = message["status"]
                process_time = (time.time() - start_time) * 1000
                console_log(f"üì§ RESPONSE ‚Üê {method} {path}")
                console_log(f"   Status: {status_code}")
                console_log(f"   Time: {process_time:.2f}ms")
            await send(message)

        await self.app(scope, receive, wrapped_send)

# Replace the old middleware with the new pure ASGI one
app.add_middleware(ASGILoggerMiddleware)


# ====== 7. ENDPOINT CH√çNH ======
@app.post("/process_query", response_model=ChatResponse)
async def process_query(request: ChatRequest):
    try:
        redis_conn = app.state.redis
        client = app.state.openai_client
        reflector = app.state.reflector
        
        user_input = request.user_input.strip()
        history = request.history[-5:]

        # 1. Router c√¢u g·ªëc
        score_raw, route_name, filter_dict = router.guide(user_input)
        score = float(score_raw.item()) if hasattr(score_raw, 'item') else float(score_raw)

        # 2. X·ª≠ l√Ω Chitchat s·ªõm
        if score > 0.7 and route_name in ("uncertain", "chitchat"):
            ans = "Ch√†o b·∫°n! M√¨nh l√† Sen. B·∫°n c·∫ßn m√¨nh gi√∫p g√¨ v·ªÅ vƒÉn h√≥a Vi·ªát Nam kh√¥ng?"
            resp = ChatResponse(answer=ans, rewritten_query=user_input, route=route_name, score=score, audio_base64=await generate_audio_async(ans))
            console_log("   Response:", resp.model_dump()) # Added manual log
            return resp

        # 3. Rewrite c√¢u h·ªèi (GPT fix kh√¥ng d·∫•u, l·ªói ch√≠nh t·∫£ ·ªü ƒë√¢y)
        rewritten = await reflector.rewrite(history, user_input)
        
        # 4. Check Cache
        cache_key = f"cache:{rewritten.lower()}"
        cached_data = await redis_conn.get(cache_key)
        if cached_data:
            logger.info("üöÄ Cache Hit!")
            data = json.loads(cached_data)
            resp = ChatResponse(**data)
            console_log("   Response:", data) # Added manual log
            return resp

        # 5. [N√ÇNG C·∫§P] HYBRID RAG PIPELINE (Vector + Keyword)
        
        # B∆∞·ªõc A: T√¨m ki·∫øm Vector (L·∫•y 10 ·ª©ng vi√™n)
        q_vec = local_embedder.encode([rewritten])[0].tolist()
        candidates = vector_db.query(COLLECTION_NAME, q_vec, limit=10, filter_dict=filter_dict)
        
        if not candidates:
            ans = "Ti·∫øc qu√°, hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ ph·∫ßn n√†y."
            resp = ChatResponse(answer=ans, rewritten_query=rewritten, route=route_name, score=score, audio_base64=await generate_audio_async(ans))
            console_log("   Response:", resp.model_dump()) # Added manual log
            return resp

        # B∆∞·ªõc B: Reranking b·∫±ng Keyword Score
        for res in candidates:
            v_score = res.get('score', 0)  # ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng t·ª´ Vector DB
            k_score = simple_keyword_score(res['content'], rewritten) # ƒêi·ªÉm kh·ªõp t·ª´ kh√≥a
            
            # T√≠nh ƒëi·ªÉm lai: Vector ƒë√≥ng g√≥p ch√≠nh, Keyword c·ªông th√™m ƒëi·ªÉm th∆∞·ªüng
            res['hybrid_score'] = v_score + (k_score * 0.15) 

        # B∆∞·ªõc C: S·∫Øp x·∫øp l·∫°i danh s√°ch theo ƒëi·ªÉm Hybrid
        candidates.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # B∆∞·ªõc D: L·∫•y Top 3 ƒëo·∫°n Context s√°t √Ω nh·∫•t
        top_3_results = candidates[:3]
        context = "\n\n".join([r["content"] for r in top_3_results])
        logger.info(f"‚úÖ Hybrid Search th√†nh c√¥ng. Top score: {candidates[0]['hybrid_score']:.2f}")

        # 6. Generate Answer (GPT-4o-mini)
        final_res = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "B·∫°n l√† AI t√™n Sen. Tr·∫£ l·ªùi th√¢n thi·ªán d·ª±a tr√™n CONTEXT ƒë∆∞·ª£c cung c·∫•p. N·∫øu kh√¥ng c√≥ trong context, h√£y xin l·ªói kh√©o l√©o."},
                {"role": "user", "content": f"CONTEXT:\n{context}\n\nQ: {rewritten}"}
            ],
            temperature=0.3
        )
        answer = final_res.choices[0].message.content
        
        audio_b64 = await generate_audio_async(answer)

        response_data = ChatResponse(
            answer=answer, rewritten_query=rewritten, route=route_name,
            score=score, audio_base64=audio_b64, context_used=context[:150] + "..."
        )

        # 7. L∆∞u cache (H·∫øt h·∫°n sau 1 ti·∫øng)
        await redis_conn.setex(cache_key, 3600, response_data.model_dump_json())
        
        console_log("   Response:", response_data.model_dump()) # Added manual log
        return response_data

    except Exception as e:
        logger.error(f"‚ùå Server Error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# 8. Endpoint Root
@app.get("/")
async def root():
    return {
        "message": "AI Sen API is running!",
        "status": "online",
        "author": "Hieu"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)