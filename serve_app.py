import os
import base64
import re
import json
import logging
from typing import List, Dict, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import AsyncOpenAI 
import redis.asyncio as redis 
from edge_tts import Communicate
from langdetect import detect
from sentence_transformers import SentenceTransformer

# C√°c b·ªô chia nh·ªè vƒÉn b·∫£n (C·∫ßn thi·∫øt cho Ingest)
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# Module t√πy ch·ªânh c·ªßa Hi·∫øu
from vector_db import VectorDatabase
from reflection import Reflection
from semantic_router.route import Route
from semantic_router.router import SemanticRouter
import semantic_router.samples as samples

# ====== 1. C·∫§U H√åNH GLOBAL ======
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DB_NAME = "vector_db"
COLLECTION_NAME = "culture"
REDIS_URL = os.getenv("REDIS_URL")

# Model 384 chi·ªÅu d√πng cho c·∫£ Router v√† Search
local_embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu
CULTURE_FILES = [
    ("mua_roi_nuoc", "mua_roi_nuoc.md"),
    ("hoang_thanh", "hoang_thanh.md"),
]

# ====== 2. H√ÄM H·ªñ TR·ª¢ N·∫†P D·ªÆ LI·ªÜU & LOGIC HYBRID ======

def simple_keyword_score(content: str, rewritten_query: str) -> float:
    """
    [M·ªöI B·ªî SUNG] 
    T√≠nh ƒëi·ªÉm th∆∞·ªüng d·ª±a tr√™n m·ª©c ƒë·ªô tr√πng l·∫∑p t·ª´ v·ª±ng gi·ªØa c√¢u h·ªèi v√† n·ªôi dung.
    """
    content_lower = content.lower()
    # T√°ch t·ª´, b·ªè qua c√°c t·ª´ qu√° ng·∫Øn (nh∆∞ 'l√†', '·ªü', 'c√≥')
    query_words = [w for w in rewritten_query.lower().split() if len(w) > 2]
    
    if not query_words:
        return 0.0
    
    matched_count = 0
    for word in query_words:
        if word in content_lower:
            matched_count += 1
            
    return matched_count / len(query_words)

def chunk_markdown(md_text: str) -> List[Dict]:
    headers_to_split_on = [("#", "h1"), ("##", "h2"), ("###", "h3")]
    splitter1 = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    sections = splitter1.split_text(md_text)
    splitter2 = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=180)
    docs = splitter2.split_documents(sections)
    return [{"content": d.page_content.strip(), "metadata": d.metadata} for d in docs if d.page_content.strip()]

def auto_ingest_data(v_db: VectorDatabase):
    """Ki·ªÉm tra v√† t·ª± ƒë·ªông n·∫°p d·ªØ li·ªáu 384 chi·ªÅu n·∫øu DB tr·ªëng"""
    for culture_type, file_path in CULTURE_FILES:
        if v_db.count_documents(COLLECTION_NAME, {"culture_type": culture_type}) == 0:
            if os.path.exists(file_path):
                logger.info(f"üîÑ ƒêang n·∫°p l·∫°i d·ªØ li·ªáu 384 chi·ªÅu cho: {culture_type}")
                with open(file_path, "r", encoding="utf-8") as f:
                    md_content = f.read()
                
                chunks = chunk_markdown(md_content)
                vectors = local_embedder.encode([c["content"] for c in chunks]).tolist()
                
                docs_to_insert = [
                    {
                        "content": c["content"],
                        "embedding": vectors[i],
                        "culture_type": culture_type,
                        "metadata": c["metadata"]
                    } for i, c in enumerate(chunks)
                ]
                v_db.insert_many(COLLECTION_NAME, docs_to_insert)
                logger.info(f"‚úÖ ƒê√£ n·∫°p xong {len(docs_to_insert)} ƒëo·∫°n cho {culture_type}")
            else:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {file_path}")

# ====== 3. ROUTER ======
routes = [
    Route(name="roi_nuoc", samples=samples.roiNuocSample, filter_dict={"culture_type": "mua_roi_nuoc"}),
    Route(name="hoang_thanh", samples=samples.hoangThanhSample, filter_dict={"culture_type": "hoang_thanh"}),
    Route(name="chitchat", samples=samples.chitchatSample, filter_dict={}),
]
router = SemanticRouter(embedding=local_embedder, routes=routes, threshold=0.5)

# ====== 4. MODELS ======
class ChatRequest(BaseModel):
    user_input: str
    history: List[Dict[str, str]]

class ChatResponse(BaseModel):
    answer: str
    rewritten_query: str
    route: str
    score: float
    audio_base64: Optional[str] = None
    context_used: Optional[str] = None

# ====== 5. LIFESPAN ======
@asynccontextmanager
async def lifespan(app: FastAPI):
    if not REDIS_URL:
        raise ValueError("‚ùå L·ªói: REDIS_URL ch∆∞a ƒë∆∞·ª£c khai b√°o trong .env")
        
    app.state.redis = redis.from_url(REDIS_URL, decode_responses=True)
    app.state.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    app.state.reflector = Reflection(llm_client=app.state.openai_client)
    
    # auto_ingest_data(vector_db)
    yield
    await app.state.redis.close()

app = FastAPI(title="Heritage NPC API", lifespan=lifespan)
vector_db = VectorDatabase(db_name=DB_NAME)

# ====== 6. UTILS ======
async def generate_audio_async(text: str) -> str:
    try:
        clean_text = re.sub(r'[*_#]', '', text).strip()
        lang = "vi-VN-HoaiMyNeural"
        try:
            if detect(clean_text[:30]) == 'en': lang = "en-US-GuyNeural"
        except: pass
        communicate = Communicate(clean_text, lang)
        audio_data = b""
        async for chunk in communicate.stream():
            if chunk["type"] == "audio": audio_data += chunk["data"]
        return base64.b64encode(audio_data).decode()
    except: return ""

# ====== 7. ENDPOINT CH√çNH ======
@app.post("/process_query", response_model=ChatResponse)
async def process_query(request: ChatRequest):
    try:
        redis_conn = app.state.redis
        client = app.state.openai_client
        reflector = app.state.reflector
        
        user_input = request.user_input.strip()
        history = request.history[-5:]

        # 1. Router c√¢u g·ªëc
        score_raw, route_name, filter_dict = router.guide(user_input)
        score = float(score_raw.item()) if hasattr(score_raw, 'item') else float(score_raw)

        # 2. X·ª≠ l√Ω Chitchat s·ªõm
        if score > 0.7 and route_name in ("uncertain", "chitchat"):
            ans = "Ch√†o b·∫°n! M√¨nh l√† Minh. B·∫°n c·∫ßn m√¨nh gi√∫p g√¨ v·ªÅ vƒÉn h√≥a Vi·ªát Nam kh√¥ng?"
            return ChatResponse(answer=ans, rewritten_query=user_input, route=route_name, score=score, audio_base64=await generate_audio_async(ans))

        # 3. Rewrite c√¢u h·ªèi (GPT fix kh√¥ng d·∫•u, l·ªói ch√≠nh t·∫£ ·ªü ƒë√¢y)
        rewritten = await reflector.rewrite(history, user_input)
        
        # 4. Check Cache
        cache_key = f"cache:{rewritten.lower()}"
        cached_data = await redis_conn.get(cache_key)
        if cached_data:
            logger.info("üöÄ Cache Hit!")
            return ChatResponse(**json.loads(cached_data))

        # 5. [N√ÇNG C·∫§P] HYBRID RAG PIPELINE (Vector + Keyword)
        
        
        # B∆∞·ªõc A: T√¨m ki·∫øm Vector (L·∫•y 10 ·ª©ng vi√™n)
        q_vec = local_embedder.encode([rewritten])[0].tolist()
        candidates = vector_db.query(COLLECTION_NAME, q_vec, limit=10, filter_dict=filter_dict)
        
        if not candidates:
            ans = "Ti·∫øc qu√°, hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ ph·∫ßn n√†y."
            return ChatResponse(answer=ans, rewritten_query=rewritten, route=route_name, score=score, audio_base64=await generate_audio_async(ans))

        # B∆∞·ªõc B: Reranking b·∫±ng Keyword Score
        for res in candidates:
            v_score = res.get('score', 0)  # ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng t·ª´ Vector DB
            k_score = simple_keyword_score(res['content'], rewritten) # ƒêi·ªÉm kh·ªõp t·ª´ kh√≥a
            
            # T√≠nh ƒëi·ªÉm lai: Vector ƒë√≥ng g√≥p ch√≠nh, Keyword c·ªông th√™m ƒëi·ªÉm th∆∞·ªüng
            res['hybrid_score'] = v_score + (k_score * 0.15) 

        # B∆∞·ªõc C: S·∫Øp x·∫øp l·∫°i danh s√°ch theo ƒëi·ªÉm Hybrid
        candidates.sort(key=lambda x: x['hybrid_score'], reverse=True)
        
        # B∆∞·ªõc D: L·∫•y Top 3 ƒëo·∫°n Context s√°t √Ω nh·∫•t
        top_3_results = candidates[:3]
        context = "\n\n".join([r["content"] for r in top_3_results])
        logger.info(f"‚úÖ Hybrid Search th√†nh c√¥ng. Top score: {candidates[0]['hybrid_score']:.2f}")

        # 6. Generate Answer (GPT-4o-mini)
        final_res = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "B·∫°n l√† NPC t√™n Minh. Tr·∫£ l·ªùi th√¢n thi·ªán d·ª±a tr√™n CONTEXT ƒë∆∞·ª£c cung c·∫•p. N·∫øu kh√¥ng c√≥ trong context, h√£y xin l·ªói kh√©o l√©o."},
                {"role": "user", "content": f"CONTEXT:\n{context}\n\nQ: {rewritten}"}
            ],
            temperature=0.3
        )
        answer = final_res.choices[0].message.content
        audio_b64 = await generate_audio_async(answer)

        response_data = ChatResponse(
            answer=answer, rewritten_query=rewritten, route=route_name,
            score=score, audio_base64=audio_b64, context_used=context[:150] + "..."
        )

        # 7. L∆∞u cache (H·∫øt h·∫°n sau 1 ti·∫øng)
        await redis_conn.setex(cache_key, 3600, response_data.model_dump_json())
        return response_data

    except Exception as e:
        logger.error(f"‚ùå Server Error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# 1. Th√™m c√°i n√†y TR∆Ø·ªöC kh·ªëi __main__
@app.get("/")
async def root():
    return {
        "message": "NPC Minh API is running!",
        "status": "online",
        "author": "Hieu"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
