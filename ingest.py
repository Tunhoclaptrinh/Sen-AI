# ingest.py
import os
import asyncio
import logging
import re
from typing import List, Dict, Any
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# LangChain Splitters & Loaders
# LangChain Splitters & Loaders (Imports moved inside functions for safety)

# Module Database & Config
from vector_db import VectorDatabase
from data_manager import get_heritage_config

# --- 1. Cáº¤U HÃŒNH ---
load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DB_NAME = "vector_db"
COLLECTION_NAME = "culture"
# Model 384 chiá»u tá»‘i Æ°u cho tiáº¿ng Viá»‡t/Ä‘a ngá»¯
local_embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

SOURCE_DIR = os.path.join(os.path.dirname(__file__), "data", "documents")

# --- 2. HELPERS: LOADERS & SMART CHUNKERS ---

def clean_text(text: str) -> str:
    """LÃ m sáº¡ch vÄƒn báº£n PDF/Docx Ä‘á»ƒ embedding tá»‘t hÆ¡n."""
    # XÃ³a nhiá»u dáº¥u xuá»‘ng dÃ²ng liÃªn tiáº¿p
    text = re.sub(r'\n+', '\n', text)
    # XÃ³a khoáº£ng tráº¯ng thá»«a
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def process_markdown(file_path):
    """
    Chiáº¿n lÆ°á»£c cho Markdown: Táº­n dá»¥ng cáº¥u trÃºc Header (#, ##, ###) Ä‘á»ƒ cáº¯t ngá»¯ nghÄ©a.
    """
    try:
        try:
            from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
        except ImportError:
            from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
            
        # 1. Cáº¯t theo cáº¥u trÃºc Header (Semantic Splitting)
        headers_to_split_on = [
            ("#", "h1"),
            ("##", "h2"),
            ("###", "h3"),
        ]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        md_header_splits = markdown_splitter.split_text(text)

        # 2. Cáº¯t má»‹n láº¡i náº¿u header content quÃ¡ dÃ i
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)
        final_docs = text_splitter.split_documents(md_header_splits)
        
        results = []
        for doc in final_docs:
            # Metadata tá»« Header Splitter ráº¥t giÃ¡ trá»‹ (h1, h2, h3)
            # Ta sáº½ Ä‘Æ°a nÃ³ vÃ o ná»™i dung embedding luÃ´n Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m
            header_context = ""
            if "h1" in doc.metadata: header_context += f"{doc.metadata['h1']}. "
            if "h2" in doc.metadata: header_context += f"{doc.metadata['h2']}. "
            if "h3" in doc.metadata: header_context += f"{doc.metadata['h3']}. "
            
            # Ná»™i dung thá»±c táº¿ Ä‘Æ°á»£c embed = Context + Content
            enriched_content = f"{header_context}\n{doc.page_content}"
            
            results.append({
                "content": clean_text(doc.page_content), # LÆ°u ná»™i dung gá»‘c sáº¡ch Ä‘áº¹p Ä‘á»ƒ hiá»ƒn thá»‹
                "embedding_content": clean_text(enriched_content), # DÃ¹ng ná»™i dung giÃ u ngá»¯ nghÄ©a Ä‘á»ƒ embed
                "metadata": {
                    "source": os.path.basename(file_path),
                    "chunk_size": len(doc.page_content),
                    **doc.metadata
                }
            })
            
        return results
    except Exception as e:
        logger.error(f"âŒ Lá»—i xá»­ lÃ½ Markdown {file_path}: {e}")
        return []

def process_file_generic(file_path: str, loader_cls) -> List[Dict]:
    """
    Chiáº¿n lÆ°á»£c cho PDF/DOCX: Recursive Splitting + Overlap sÃ¢u (1200/250).
    """
    try:
        try:
            from langchain_text_splitters import RecursiveCharacterTextSplitter
        except ImportError:
            from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        loader = loader_cls(file_path)
        raw_docs = loader.load()
        
        # Merge táº¥t cáº£ page thÃ nh 1 text lá»›n Ä‘á»ƒ trÃ¡nh Ä‘á»©t gÃ£y cÃ¢u giá»¯a cÃ¡c trang (PDF footer issue)
        full_text = "\n".join([d.page_content for d in raw_docs])
        cleaned_text = clean_text(full_text)
        
        # Cáº¯t Ä‘oáº¡n: Chunk size lá»›n hÆ¡n MD vÃ¬ vÄƒn báº£n thÆ°á»ng liÃªn tá»¥c
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=250, # Overlap lá»›n Ä‘á»ƒ giá»¯ máº¡ch vÄƒn
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_text(cleaned_text)
        
        results = []
        for i, chunk in enumerate(chunks):
            results.append({
                "content": chunk,
                "embedding_content": chunk, # PDF khÃ´ng cÃ³ header structured nhÆ° MD
                "metadata": {
                    "source": os.path.basename(file_path),
                    "page_estimated": i // 3 + 1 # Æ¯á»›c lÆ°á»£ng trang
                }
            })
        return results
    except Exception as e:
        logger.error(f"âŒ Lá»—i xá»­ lÃ½ file {file_path}: {e}")
def process_pdf_advanced(file_path: str) -> List[Dict]:
    """
    Advanced PDF Processor sá»­ dá»¥ng `pdfplumber` Ä‘á»ƒ giá»¯ layout tá»‘t hÆ¡n.
    """
    try:
        import pdfplumber
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        full_text = ""
        logger.info(f"ðŸ“„ Äang Ä‘á»c PDF báº±ng pdfplumber: {os.path.basename(file_path)}")
        
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                # extract_text(x_tolerance=1) giÃºp join text cÃ¹ng dÃ²ng tá»‘t hÆ¡n
                text = page.extract_text(x_tolerance=2, y_tolerance=2) 
                if text:
                    full_text += text + "\n\n"
        
        cleaned_text = clean_text(full_text)
        
        # Chunking: Chiáº¿n lÆ°á»£c Overlap sÃ¢u cho PDF
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=250,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        chunks = text_splitter.split_text(cleaned_text)
        
        results = []
        for i, chunk in enumerate(chunks):
            results.append({
                "content": chunk,
                "embedding_content": chunk, 
                "metadata": {
                    "source": os.path.basename(file_path),
                    "page_estimated": i // 3 + 1
                }
            })
        return results

    except ImportError:
        logger.error("âŒ ChÆ°a cÃ i pdfplumber. Fallback sang generic loader.")
        return []
    except Exception as e:
        logger.error(f"âŒ Custom PDF Error: {e}")
        return []

# --- 3. MAIN INGEST FLOW ---

async def ingest():
    v_db = VectorDatabase(db_name=DB_NAME)
    logger.info("ðŸš€ Báº¯t Ä‘áº§u Smart Ingest (MD, PDF, DOCX)...")

    try:
        from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
    except ImportError:
        logger.error("âŒ Thiáº¿u 'langchain-community'. Bá» qua file PDF/DOCX.")
        PyPDFLoader = None
        Docx2txtLoader = None
    
    # 1. Load config Ä‘á»ƒ láº¥y danh sÃ¡ch di tÃ­ch (ChÃºng ta sáº½ map file vá»›i di tÃ­ch qua tÃªn file hoáº·c thÆ° má»¥c)
    config = get_heritage_config()
    
    # 2. QuÃ©t thÆ° má»¥c data/documents (Náº¿u chÆ°a cÃ³ thÃ¬ táº¡o)
    if not os.path.exists(SOURCE_DIR):
        os.makedirs(SOURCE_DIR)
        logger.info(f"ðŸ“‚ ÄÃ£ táº¡o thÆ° má»¥c '{SOURCE_DIR}'. HÃ£y bá» file .md/.pdf/.docx vÃ o Ä‘Ã¢y (Ä‘áº·t tÃªn trÃ¹ng site_key, vd: hoang_thanh.pdf).")
        
    # Láº¥y danh sÃ¡ch file trong thÆ° má»¥c
    files = os.listdir(SOURCE_DIR)
    
    total_chunks = 0
    
    for filename in files:
        file_path = os.path.join(SOURCE_DIR, filename)
        if not os.path.isfile(file_path): continue
        
        # XÃ¡c Ä‘á»‹nh site_key qua tÃªn file (vd: hoang_thanh_v2.pdf -> hoang_thanh)
        # Logic Ä‘Æ¡n giáº£n: Láº¥y pháº§n Ä‘áº§u tÃªn file lÃ m key. Cáº§n khá»›p vá»›i config.json
        matched_key = None
        matched_culture_type = "di_tich"
        
        for key in config.keys():
            if key in filename:  # Logic linh hoáº¡t: Chá»‰ cáº§n tÃªn file CHá»¨A key lÃ  Ä‘Æ°á»£c
                matched_key = key
                matched_culture_type = config[key].get("culture_type", "di_tich")
                break
        
        if not matched_key:
            logger.warning(f"â© File '{filename}' khÃ´ng khá»›p key nÃ o trong monuments.json (VD: pháº£i chá»©a 'hoang_thanh'). Bá» qua.")
            continue

        # Get collection from config (default to 'culture' if not found)
        target_collection = config[matched_key].get("collection", COLLECTION_NAME)

        # Check trÃ¹ng trong DB
        existing_count = v_db.count_documents(target_collection, {
            "metadata.site_key": matched_key, 
            "metadata.source": filename
        })
        if existing_count > 0:
            logger.info(f"ðŸ”„ File '{filename}' Ä‘Ã£ náº¡p. XÃ³a báº£n cÅ© Ä‘á»ƒ náº¡p báº£n má»›i...")
            v_db.collection.delete_many({
                "metadata.site_key": matched_key, 
                "metadata.source": filename
            })
            
        logger.info(f"ðŸ“„ Äang xá»­ lÃ½ file: {filename} (Site: {matched_key}) -> Collection: {target_collection}...")
        
        # Xá»­ lÃ½ theo loáº¡i file
        ext = os.path.splitext(filename)[1].lower()
        processed_docs = []
        
        if ext == ".md":
            processed_docs = process_markdown(file_path)
        elif ext == ".pdf":
            if PyPDFLoader: processed_docs = process_file_generic(file_path, PyPDFLoader)
        elif ext == ".docx":
            if Docx2txtLoader: processed_docs = process_file_generic(file_path, Docx2txtLoader)
        else:
            continue
            
        if not processed_docs:
            logger.warning(f"âš ï¸ KhÃ´ng trÃ­ch xuáº¥t Ä‘Æ°á»£c ná»™i dung tá»« {filename}.")
            continue
            
        # 3. Embedding & Insert
        to_insert_list = []
        
        # Batch embedding Ä‘á»ƒ nhanh hÆ¡n
        embed_texts = [d["embedding_content"] for d in processed_docs]
        vectors = local_embedder.encode(embed_texts).tolist()
        
        # Determine the dynamic type field based on collection
        # Default to 'culture_type' if unknown
        type_field_map = {
            "culture": "culture_type",
            "heritage": "heritage_type",
            "history": "history_type"
        }
        dynamic_type_field = type_field_map.get(target_collection, "culture_type")
        
        for i, doc in enumerate(processed_docs):
            json_doc = {
                "content": doc["content"],
                "embedding": vectors[i],
                dynamic_type_field: matched_key, # Dynamic field: heritage_type='hoang_thanh'
                "metadata": {
                    "site_key": matched_key,
                    "source": filename,
                    "file_type": ext,
                    **doc["metadata"]
                }
            }
            to_insert_list.append(json_doc)
            
        if to_insert_list:
            v_db.insert_many(target_collection, to_insert_list)
            total_chunks += len(to_insert_list)
            logger.info(f"âœ… ÄÃ£ thÃªm {len(to_insert_list)} chunks tá»« {filename}.")

    # --- NGUá»’N PHá»¤: Náº P Tá»ª MONUMENTS.JSON DESCRIPTION (Náº¿u chÆ°a cÃ³ file chi tiáº¿t) ---
    logger.info("--- Kiá»ƒm tra mÃ´ táº£ ngáº¯n trong monuments.json ---")
    for key, data in config.items():
        # Get target collection & type field
        target_col = data.get("collection", COLLECTION_NAME)
        
        # Determine dynamic type field
        type_field_map = {
            "culture": "culture_type",
            "heritage": "heritage_type",
            "history": "history_type"
        }
        dynamic_type_field = type_field_map.get(target_col, "culture_type")

        # Kiá»ƒm tra xem site nÃ y Ä‘Ã£ cÃ³ data tá»« file document chÆ°a
        doc_count = v_db.count_documents(target_col, {"metadata.site_key": key, "metadata.file_type": {"$in": [".pdf", ".md", ".docx"]}})
        
        if doc_count > 0:
            continue # Æ¯u tiÃªn file document chi tiáº¿t hÆ¡n description ngáº¯n
            
        # Check description existing
        desc_count = v_db.count_documents(target_col, {"metadata.site_key": key, "metadata.source": "monuments.json"})
        if desc_count > 0: continue
        
        desc = data.get("context_description", "")
        if desc:
            logger.info(f"ðŸ“ Náº¡p mÃ´ táº£ ngáº¯n cho '{data['name']}' vÃ o '{target_col}'...")
            # Embed description
            full_text = f"{data['name']}.\n{desc}"
            vec = local_embedder.encode([full_text]).tolist()[0]
            
            v_db.insert_many(target_col, [{
                "content": desc,
                "embedding": vec,
                dynamic_type_field: key,
                "metadata": {"site_key": key, "source": "monuments.json", "level": 0}
            }])
            total_chunks += 1

    logger.info(f"âœ¨ HoÃ n táº¥t! Tá»•ng cá»™ng thÃªm {total_chunks} chunks má»›i.")

async def ingest_file(file_path: str, site_key: str):
    """
    API Helper: Ingest má»™t file cá»¥ thá»ƒ cho di tÃ­ch cá»¥ thá»ƒ.
    """
    v_db = VectorDatabase(db_name=DB_NAME)
    filename = os.path.basename(file_path)
    
    # Get config for this site
    config = get_heritage_config()
    site_config = config.get(site_key, {})
    target_collection = site_config.get("collection", COLLECTION_NAME)
    
    # Determine dynamic type field
    type_field_map = {
        "culture": "culture_type",
        "heritage": "heritage_type",
        "history": "history_type"
    }
    dynamic_type_field = type_field_map.get(target_collection, "culture_type")
    
    # 1. CÆ  CHáº¾ OVERWRITE: Kiá»ƒm tra vÃ  xÃ³a dá»¯ liá»‡u cÅ©
    existing_count = v_db.count_documents(target_collection, {
        "metadata.site_key": site_key, 
        "metadata.source": filename
    })
    
    if existing_count > 0:
        logger.info(f"ðŸ”„ File '{filename}' Ä‘Ã£ tá»“n táº¡i trong '{target_collection}'. Äang xÃ³a {existing_count} chunks cÅ©...")
        # XÃ³a chunks cÅ© (Using raw collection access or a new method if available, defaulting to raw)
        # Note: v_db wrapper usually handles one collection. We need to be careful if v_db is tied to one collection.
        # Looking at v_db init: `self.collection = db[collection_name]`. 
        # But we are calling methods with `collection_name` arg in `ingest()` loop?
        # Let's check `count_documents` signature. It takes `collection_name`.
        # So we should use `db[target_collection].delete_many`.
        v_db.db[target_collection].delete_many({
            "metadata.site_key": site_key, 
            "metadata.source": filename
        })
        logger.info("ðŸ—‘ï¸ ÄÃ£ xÃ³a dá»¯ liá»‡u cÅ©.")

    # Process
    ext = os.path.splitext(filename)[1].lower()
    processed_docs = []
    
    # Lazy Import Loaders
    try:
        # from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
        from langchain_community.document_loaders.word_document import Docx2txtLoader
        from langchain_community.document_loaders.pdf import PyPDFLoader
    except ImportError:
         return {"status": "error", "message": "Thiáº¿u thÆ° viá»‡n 'langchain-community'. HÃ£y cÃ i Ä‘áº·t!"}

    if ext == ".md":
        processed_docs = process_markdown(file_path)
    elif ext == ".pdf":
        if PyPDFLoader: processed_docs = process_file_generic(file_path, PyPDFLoader)
    elif ext == ".docx":
        if Docx2txtLoader: processed_docs = process_file_generic(file_path, Docx2txtLoader)
    
    if not processed_docs:
        return {"status": "error", "message": f"KhÃ´ng Ä‘á»c Ä‘Æ°á»£c ná»™i dung file {filename}."}
        
    # Embed & Insert
    embed_texts = [d["embedding_content"] for d in processed_docs]
    vectors = local_embedder.encode(embed_texts).tolist()
    
    to_insert_list = []
    
    for i, doc in enumerate(processed_docs):
        json_doc = {
            "content": doc["content"],
            "embedding": vectors[i],
            dynamic_type_field: site_key, # Dynamic filter key
            "metadata": {
                "site_key": site_key,
                "source": filename,
                "file_type": ext,
                **doc["metadata"]
            }
        }
        to_insert_list.append(json_doc)
        
    if to_insert_list:
        v_db.insert_many(target_collection, to_insert_list)
        return {"status": "success", "chunks": len(to_insert_list)}
        
    return {"status": "error", "message": "KhÃ´ng cÃ³ chunk nÃ o Ä‘Æ°á»£c táº¡o."}

async def ingest_file_to_collection_advanced(
    file_path: str, 
    collection_name: str, 
    culture_type: str,
    culture_type_name: str,
    ingest_mode: str = "append" # "append" or "replace"
):
    print(" >>>>>>>>> DEBUG: Started Ingest Function <<<<<<<<< ") 
    """
    [ADVANCED] Ingest file vÃ o collection tÃ¹y chá»n vá»›i culture_type cá»¥ thá»ƒ.
    """
    # 0. Init DB
    v_db = VectorDatabase(db_name=DB_NAME)
    
    filename = os.path.basename(file_path)
    ext = os.path.splitext(filename)[1].lower()
    
    logger.info(f"ðŸ“¥ [ADVANCED INGEST] File: {filename} â†’ Collection: {collection_name}, Type: {culture_type}, Mode: {ingest_mode}")
    
    # 1. Loaders
    try:
        from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
    except ImportError:
         return {"status": "error", "message": "Thiáº¿u thÆ° viá»‡n 'langchain-community'. HÃ£y cÃ i Ä‘áº·t!"}

    # 2. Parse document
    print(f"DEBUG: Parsing {ext} file...")
    processed_docs = []
    try:
        if ext == '.pdf':
            # Æ¯u tiÃªn dÃ¹ng Advanced PDF Processor (Layout-aware)
            try: 
                 processed_docs = process_pdf_advanced(file_path)
            except: pass
            
            # Fallback náº¿u advanced fail
            if not processed_docs:
                logger.warning("Falling back to standard PDF loader...")
                processed_docs = process_file_generic(file_path, PyPDFLoader)
                
        elif ext == '.docx':
            processed_docs = process_file_generic(file_path, Docx2txtLoader)
        elif ext == '.md':
            processed_docs = process_markdown(file_path)
        else:
            return {"status": "error", "message": f"Unsupported file type: {ext}"}
        
        logger.info(f"âœ… Parsed {len(processed_docs)} docs from {filename}")

    except Exception as e:
         logger.error(f"âŒ Parse Error: {e}")
         return {"status": "error", "message": f"Parse Error: {e}"}
    
    if not processed_docs:
        return {"status": "error", "message": "KhÃ´ng parse Ä‘Æ°á»£c ná»™i dung tá»« file."}
    
    # Determine dynamic type field
    type_field_map = {
        "culture": "culture_type",
        "heritage": "heritage_type",
        "history": "history_type"
    }
    dynamic_type_field = type_field_map.get(collection_name, "culture_type")

    # DELETE EXISTING IF "REPLACE" MODE
    if ingest_mode == "replace":
        if hasattr(v_db, 'delete_many'):
             try:
                 deleted_count = v_db.delete_many(collection_name, {dynamic_type_field: culture_type})
                 logger.info(f"ðŸ—‘ï¸ Deleted {deleted_count} existing docs for {dynamic_type_field}={culture_type} in {collection_name}")
             except Exception as e:
                 logger.error(f"âŒ Delete Error: {e}")
        else:
             logger.warning("Warning: v_db.delete_many not found. Skipping delete.")

    # Embed
    try:
        embed_texts = [doc["embedding_content"] for doc in processed_docs]
        vectors = local_embedder.encode(embed_texts).tolist()
        logger.info(f"âœ… Created {len(vectors)} embeddings")
    except Exception as e:
        logger.error(f"âŒ Embedding Error: {e}")
        return {"status": "error", "message": f"Embedding Error: {e}"}

    # Build documents
    to_insert = []
    for i, doc in enumerate(processed_docs):
        json_doc = {
            "content": doc["content"],
            "embedding": vectors[i],
            dynamic_type_field: culture_type,  
            "metadata": {
                "site_key": culture_type,  
                "source": filename,
                "file_type": ext,
                "ingest_time": os.getenv("INGEST_ID", "manual"),
                **doc["metadata"]
            }
        }
        to_insert.append(json_doc)
    
    # Insert vÃ o collection (dynamic)
    inserted_count = 0
    if to_insert:
        logger.info(f"ðŸš€ Inserting {len(to_insert)} chunks into MongoDB Collection '{collection_name}'...")
        try:
            inserted_count = v_db.insert_many(collection_name, to_insert)
            logger.info(f"âœ… Inserted {inserted_count} docs into {collection_name}")
        except Exception as e:
            logger.error(f"âŒ DB Insert Error: {e}")
            return {"status": "error", "message": f"DB Insert Error: {e}"}

    return {"status": "success", "chunks": inserted_count, "mode": ingest_mode}

if __name__ == "__main__":
    asyncio.run(ingest())